<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="1.11的任务：  看综述*2：od,然后写相关的阅读报告，规划一下怎么做那个事儿。 规划一下SE大作业的提交 日语课*5：先复习一下初日上的课开始吧 入党材料整理归档：主要还剩下什么材料需要弄？  Object Detection   image-20250111224456852   one-stage: CNN网络进行回归，YOLO系列 two-stage：Fas">
<meta property="og:type" content="article">
<meta property="og:title" content="2025-1-11-Object_Detection">
<meta property="og:url" content="http://example.com/2025/01/10/2025-1-11-Object-Detection/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="1.11的任务：  看综述*2：od,然后写相关的阅读报告，规划一下怎么做那个事儿。 规划一下SE大作业的提交 日语课*5：先复习一下初日上的课开始吧 入党材料整理归档：主要还剩下什么材料需要弄？  Object Detection   image-20250111224456852   one-stage: CNN网络进行回归，YOLO系列 two-stage：Fas">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113104859576.png">
<meta property="article:published_time" content="2025-01-10T15:22:26.000Z">
<meta property="article:modified_time" content="2025-01-13T05:11:38.603Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="目标检测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113104859576.png"><title>2025-1-11-Object_Detection - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>2025-1-11-Object_Detection</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2025-01-10</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-01-13</time></div>
<div class="post-categories"><i class="icon fa-regular fa-folder-open" title="分类"></i><a class="post-category" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div>
<div class="post-tags"><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约5.1K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><p>1.11的任务：</p>
<ul class="task-list">
<li><input
type="checkbox" />看综述*2：od,然后写相关的阅读报告，规划一下怎么做那个事儿。</li>
<li><input type="checkbox" />规划一下SE大作业的提交</li>
<li><input type="checkbox"
checked="" />日语课*5：先复习一下初日上的课开始吧</li>
<li><input
type="checkbox" />入党材料整理归档：主要还剩下什么材料需要弄？</li>
</ul>
<h1 id="object-detection">Object Detection</h1>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250111224456852.png"
alt="image-20250111224456852" />
<figcaption aria-hidden="true">image-20250111224456852</figcaption>
</figure>
<ul>
<li>one-stage: CNN网络进行回归，YOLO系列</li>
<li>two-stage：Faster-RCNN等(两阶段多了预选步骤)</li>
</ul>
<h2 id="two-stage">Two-Stage</h2>
<p><strong>第一阶段</strong>：生成候选区域（Region
Proposals），即识别可能包含目标的位置。</p>
<p><strong>第二阶段</strong>：对这些候选区域进行分类和边界框回归。</p>
<h3 id="rcnn1-2014">RCNN[1]-2014</h3>
<p><strong>方法</strong>：选择性搜索(Select Serach,SS) obj candidate
boxes中的一些可能的对象候选框、然后对这些框resize成同样的大小、然后使用pre-train
CNN(on ImageNet)上提取特征、最后使用Linear
SVM来预测每个候选框内的对象是否存在、并进行识别。</p>
<blockquote>
<p><strong>选择性搜索算法</strong>：为了提升区域选择效率而使用的算法，该算法相较于穷举法有较大提升，其本质是图像分割算法。首先初始化区域集，计算相邻区域相似度，具有高相似度区域合并到一起。具体的相似度计算分为四个子模块（颜色、纹理、尺度、填充）。颜色和纹理是判断区域相似度的基本要素；尺度相似度则是为了避免合并后的区域不断吞并周围区域，造成无效的区域划分；填充相似度则是为了解决物体间的包含关系（如：车轮在汽车上、眼睛在动物身上）。最终将四个模块归一化求和计算。</p>
<p><strong>CNN</strong>：用于提取候选区域的图像特征，这一步中可以采用的卷积神经网络可以有很多种（Resnet、Inception、VGG等），最初的RCNN仅提取网络的高层特征，即被多次下采样后的高级抽象特征。因此无法很好的提取小目标。</p>
<p><strong>SVM</strong>：用于对特征进行类别分类，使用n个svm组成的权值矩阵（n为分类数）与特征矩阵相乘，得到最终的概率矩阵。使用非极大抑制对每类样本的进行处理剔除重叠建议框，保留高分框。SVM本质上是二分类分类器，通过核技巧将特征映射到高维空间来处理线性不可分样本，通过叠加多个SVM分类器可以处理这样的多分类问题（也称为One-vs-All
(OvA) 或 One-vs-Rest 方法）。</p>
<p><strong>线性回归模型</strong>：用于微调非极大抑制筛选后的边界框。在训练期间，通过比较预测的边界框和真实目标边界框之间的差异来计算回归损失。回归损失用于调整回归器的权重，以使其能够更准确地预测边界框的位置。</p>
</blockquote>
<p><strong>效果：</strong>VOC07-mAP:33.7%-&gt;58.5%</p>
<p><strong>缺点：</strong>速度超慢。原因是overlap的候选框太多了、每个都要计算。一张图会产生近2,000个候选框，一张图片运行在GPU上需要14s;此外，需要分开训练CNN，因此无法做到端到端训练；因此推理速度极慢且占用内存大。</p>
<blockquote>
<p>SPPNet解决了此问题。</p>
</blockquote>
<h3 id="sppnet2-2014">SPPNet[2]-2014</h3>
<p><strong>方法：</strong>引入空间金字塔池化层(Spatial Pyramid Pooling
Layer,SPP)，让CNN不用固定输入的大小，都能输出一致长度。特征图只会被计算一次、然后生成的固定长度的任意区域的表征来训练CNN，避免重复计算卷积。</p>
<p><strong>效果：</strong>比RCNN快20倍以上、VOC07-mAP:59.2%</p>
<p><strong>缺点：</strong>SPPNet只是微调(fine-tune)了卷积层，忽略了前面的层；是多阶段训练(multi-stage
training)</p>
<h3 id="fast-rcnn3-2015">Fast RCNN[3]-2015</h3>
<p><strong>基本框架</strong>：选择性搜索算法(SS)+CNN+ROI
pooling+softmax+线性回归模型</p>
<p><strong>方法：</strong>在同一个网络框架下，同时训练检测器和边界框回归器。RCNN中将每个感兴趣区域转换成相同尺寸再使用CNN提取特征，每个感兴趣区域分别提取特征消耗计算资源极大。fast-RCNN将整张图片输入CNN获得整体特征，再将选择性搜索算法的感兴趣区域投影到图像中，获得相应的特征矩阵。然后将每个特征矩阵经ROI
pooling缩放到7*7大小特征图，特征图再输入进softmax分类器与线性回归模型中对类别和边界框位置进行预测。在训练和预测速度上相较于RCNN有较大提升。</p>
<blockquote>
<p><strong>ROI
pooling</strong>：实际上是将图像每个维度划分为7*7的区域，对每个区域执行最大池化。</p>
<p><strong>损失计算</strong>：不同RCNN训练时分开计算边界框和分类损失，fast-RCNN将特征图并联输入到softmax分类器与线性回归模型中，因此将分类损失与边界框损失的和作为最终损失值。</p>
</blockquote>
<p><strong>效果：</strong>比RCNN快200倍以上、VOC07-mAP：58.8%-&gt;70.0%</p>
<p><strong>缺点：</strong>选择性搜索仍然较为耗时，对小目标的检测性能仍然较差。训练速度被proposal
detection限制、Faster RCNN解决。</p>
<h3 id="faster-rcnn4-2015">Faster RCNN[4]-2015</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113103217520.png"
alt="image-20250113103217520" />
<figcaption aria-hidden="true">image-20250113103217520</figcaption>
</figure>
<p><strong>基本框架</strong>：CNN+RPN+ROI
pooling+softmax+线性回归模型</p>
<p><strong>改进</strong>：使用RPN替换选择性搜索（并不是指在原作用位置直接替换），其他结构与fast-RCNN无异。因为使用RPN输出边界框，无论是算法速度还是生成边界框的数量，都极大缩减了预测时间。</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113103242024.png"
alt="image-20250113103242024" />
<figcaption aria-hidden="true">image-20250113103242024</figcaption>
</figure>
<p>RPN：不同于SS作用于原图，RPN作用在特征图，因此RPN中一个滑块中对应的原图作用面积会是anchor面积的数倍，这个倍数取决于CNN的卷积以及池化层的分布情况。具体来说每个滑块中的anchor对应到原图上的尺度有三种（<code>128^2</code>,<code>256^2</code>,<code>512^2</code>）比例也有三种(1:1,1:2,2:1)。RPN会在特征图中历遍所有像素，但不是所有anchor都会被保留，对于VGG作为backbone的情况只保留256个anchor，只保留IOU大于0.7（或IOU最大）与IOU小于0.3的anchor。</p>
<p>损失计算：论文中采用分部式计算，也可以直接采用联合训练方法。分部式计算主要是将RPN的训练与fast-RCNN训练分开，先训练RPN参数，再用RPN生成边界框训练fast-RCNN，然后使用fast-RCNN的训练参数微调RPN参数以微调边界框，最后再微调fast-RCNN全连接层参数。</p>
<h3 id="fpn5-2017">FPN[5]-2017</h3>
<p>Feature Pyramid Networks</p>
<blockquote>
<p><strong>Pyramid
Networks</strong>:使用图像金字塔构建特征金字塔，每个尺度单独计算特征。虽然可以获得不同尺度的特征，但是计算资源占用大，且推理时间长。</p>
<p><strong>RCNN系列</strong>:对图像进行卷积池化操作后只提取最后一层的高级语义特征，因此无法很好的提取小目标，但它的推理速度快且计算资源占用少。</p>
<p>下图中(c)是使用金字塔特征分级，是将a和b两种方法的长处结合，将图像卷积池化后的特征分别计算，得到不同尺度的特征。兼具速度与特征量。但是特征的鲁棒性不强，浅层特征较弱，割裂计算无法发挥这些浅层特征的作用。</p>
</blockquote>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113104431028.png"
alt="image-20250113104431028" />
<figcaption aria-hidden="true">image-20250113104431028</figcaption>
</figure>
<p>(d)FPN方法本尊，使用了更深的层来构造特征金字塔，这样做是为了使用更加鲁棒的信息；除此之外，我们将处理过的低层特征和处理过的高层特征进行累加，这样做的目的是因为低层特征可以提供更加准确的位置信息，而多次的降采样和上采样操作使得深层网络的定位信息存在误差，因此我们将其结合其起来使用，就构建了一个更深的特征金字塔，融合了多层特征信息，并在不同的特征进行输出。</p>
<p><strong>方法</strong>：先将除第一层外的各层用<code>1*1*256</code>卷积统一通道数，将统一通道数后的各层从最高层开始，最高层经上采样后维数与下一层一致，将最高层特征直接与下一层相加，以此类推，得到每层的输出特征，具体流程看下图。注意这里还使用了maxpool池化生成了p6特征，p2~p6会输入到rpn中，分别使用不同尺寸的anchor（<code>32^2</code>,<code>64^2</code>,<code>128^2</code>,<code>256^2</code>,<code>512^2</code>）进行训练。分别对应识别最小到最大的目标。</p>
<p><img src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113104859576.png" alt="image-20250113104859576" style="zoom:80%;" /></p>
<p><strong>效果：</strong>COCO mAP:31.6%-&gt;33.9%. 在 ResNet-50 和
ResNet-101
骨干网络上，这个框架加进来的格外计算开销小、训练时间和推理速度相比基础
Faster R-CNN 基本保持一致，但检测性能显著提高。</p>
<p><strong>缺点</strong></p>
<ol type="1">
<li>对小目标的依赖：对极小目标的处理仍然存在局限，依赖更高分辨率的输入。</li>
<li>复杂性增加：自顶向下路径和横向连接的引入，增加了一定的实现复杂度。</li>
<li>资源需求：在较深的网络（如 ResNet-101）上，内存需求较高。</li>
</ol>
<h3 id="mask-rcnn6-2017">Mask-RCNN[6]-2017</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113103119066.png"
alt="image-20250113103119066" />
<figcaption aria-hidden="true">image-20250113103119066</figcaption>
</figure>
<p>Mask R-CNN 是 Faster R-CNN
的扩展，增加了一个用于实例分割的分支，使模型能够同时预测目标的类别、边界框和精确的像素级分割掩码。</p>
<p><strong>框架</strong>：</p>
<ol type="1">
<li><strong>两阶段检测架构</strong>：
<ul>
<li>第一阶段：区域建议网络（RPN）生成候选框。</li>
<li>第二阶段：对候选框进行分类、边界框回归，并生成对应的分割掩码。</li>
</ul></li>
<li><strong>新增分割分支</strong>：
<ul>
<li>在每个候选区域（RoI）上，Mask R-CNN
使用全卷积网络（FCN）生成固定尺寸的分割掩码（如 28×28）。</li>
</ul></li>
<li><strong>RoIAlign 对齐</strong>：
<ul>
<li>引入 RoIAlign 操作，解决了 Faster R-CNN 中 RoIPool
的量化误差问题，实现更精确的像素对齐。</li>
<li>RoIAlign 使用双线性插值进行特征提取，大幅提高了分割精度。</li>
</ul></li>
<li><strong>独立的掩码和类别预测</strong>：
<ul>
<li>Mask R-CNN
为每个类别独立预测掩码，而不是通过多类别分类交叉熵预测掩码，从而避免类别间的竞争。</li>
</ul></li>
</ol>
<p><strong>改进：</strong>增加分割分支，实现了目标检测和实例分割的统一框架；掩码预测和类别预测独立进行，避免类别间的冲突，提高了分割精度。</p>
<p><strong>实验结果：</strong>ResNet-101-FPN：Mask mAP = 35.7%，AP50 =
58.0%，AP75 = 37.8%</p>
<ul>
<li>使用 ResNet-101-FPN 骨干网络，Mask R-CNN 的推理速度为 5
fps，训练时间约为 1-2 天（8 张 GPU）。</li>
<li>Mask 分支只增加了约 20% 的计算开销。</li>
</ul>
<p><strong>缺点：</strong>在高分辨率图像和深度网络上，训练和推理的内存需求较大；对于密集目标的分割任务，仍可能存在性能瓶颈。</p>
<h2 id="one-stage">One-Stage</h2>
<p>核心：速度快，适于实时检测，基于视频来做的话，速度要求快。</p>
<p>缺点：比起two-stage来说，效果不会来好。</p>
<p>最快的时候可达200FPS</p>
<h3 id="yolo7-2016">YOLO[7]-2016</h3>
<p>使用单个神经网络应用到完整的图片中。</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125602324.png"
alt="image-20250113125602324" />
<figcaption aria-hidden="true">image-20250113125602324</figcaption>
</figure>
<p><strong>方法：</strong>提出
YOLO，一个端到端的单阶段目标检测框架，将目标检测作为回归问题统一建模。输入图像直接映射到类别概率和边界框，利用全卷积网络（CNN）实现快速推理。YOLO
将图像划分为 S×S 网格，每个网格预测目标类别和边界框。</p>
<p><strong>效果</strong>：比两阶段方法（如 Faster R-CNN）快 100
倍以上。</p>
<ul>
<li>fast version:VOC07-mAP:52.7%,155fps</li>
<li>enhanced version:VOC07-mAP:63.4%,45fps</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对小目标检测不友好，特别是在密集目标场景中，检测性能有限。<br />
</li>
<li>难以处理目标之间的重叠区域。</li>
</ul>
<h3 id="retinanet8-2017">RetinaNet[8]-2017</h3>
<p><strong>效果</strong>：VOC07-mAP@.5:59.1%, mAP@[.5,.95]:39.1%</p>
<p><strong>方法：</strong>提出 Focal Loss，用于解决单阶段检测模型（如
RetinaNet）中正负样本不平衡问题。通过对易分类样本降低权重，使模型更专注于难分类样本，提高小目标的检测性能。</p>
<p><strong>缺点：</strong></p>
<ul>
<li>增加了损失函数的复杂性。<br />
</li>
<li>在极度不平衡的数据分布中需要仔细调整超参数。</li>
</ul>
<h3 id="efficientdet9-2020">EfficientDet[9]-2020</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125638314.png"
alt="image-20250113125638314" />
<figcaption aria-hidden="true">image-20250113125638314</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125652342.png"
alt="image-20250113125652342" />
<figcaption aria-hidden="true">image-20250113125652342</figcaption>
</figure>
<p><strong>方法：</strong>提出 EfficientDet，通过引入
BiFPN（加权双向特征金字塔网络）和复合缩放策略，统一优化网络的分辨率、深度和宽度，从而在多个算力约束下实现最优检测性能。<br />
<strong>效果：</strong>COCO2017-AP:
52.2%（EfficientDet-D7），比传统方法效率高 4-9 倍，FLOPs 减少 13-42
倍。</p>
<p><strong>缺点：</strong></p>
<ul>
<li>对于非常高分辨率的图像，模型推理时间仍然较高。<br />
</li>
<li>BiFPN 的复杂性使其在小型设备上的应用受到限制。</li>
</ul>
<h3 id="swin-transformer10-2021">Swin Transformer[10]-2021</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125858331.png"
alt="image-20250113125858331" />
<figcaption aria-hidden="true">image-20250113125858331</figcaption>
</figure>
<p><strong>方法：</strong>提出 Swin Transformer，将视觉任务中的
Transformer
架构扩展为分层结构，使用滑动窗口机制，局部关注提高效率，分层设计增强全局感受野。<br />
<strong>效果：</strong>COCO-mAP: 53.5%</p>
<p><strong>缺点：</strong></p>
<ul>
<li><p>对硬件的显存要求较高。</p></li>
<li><p>训练时间较长，计算复杂度仍高于普通卷积网络。</p></li>
</ul>
<h3 id="vitae11-2021">ViTAE[11]-2021</h3>
<p><strong>方法：</strong>提出 VITAE，结合卷积网络的归纳偏置与
Transformer
的强建模能力，通过结构设计实现局部特征提取与全局表示的平衡。<br />
<strong>效果：</strong>COCO-AP:
55.2%，在检测和分割任务中优于传统方法。<br />
<strong>缺点：</strong></p>
<ul>
<li>参数量大，硬件需求高。<br />
</li>
<li>对稀疏数据分布的适应性有限。</li>
</ul>
<h3 id="dino12-2022">DINO[12]-2022</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125837880.png"
alt="image-20250113125837880" />
<figcaption aria-hidden="true">image-20250113125837880</figcaption>
</figure>
<p><strong>方法：</strong>提出 DINO，通过改进 DEtection TRansformer
(DETR)，引入降噪锚框机制（Denoising Anchor
Boxes），加快模型的收敛速度，提高对目标边界框的预测精度。<br />
<strong>效果：</strong>COCO-AP: 56.0%，显著提升了 DETR 的训练效率。</p>
<p><strong>缺点：</strong></p>
<ul>
<li>对超参数敏感，调整较为困难。<br />
</li>
<li>对长尾分布的目标检测仍存在一定不足。</li>
</ul>
<h3 id="beit1314-20212023">BEiT[13,14]-2021/2023</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125822245.png"
alt="image-20250113125822245" />
<figcaption aria-hidden="true">image-20250113125822245</figcaption>
</figure>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125803627.png"
alt="image-20250113125803627" />
<figcaption aria-hidden="true">image-20250113125803627</figcaption>
</figure>
<p><strong>方法：</strong>提出 BEiT，将语言模型中的 BERT
预训练思想引入视觉任务，通过 Masked Image Modeling (MIM)
进行自监督预训练，生成通用图像表示。<br />
<strong>效果：</strong>ImageNet Top-1 Accuracy: 85.2%。<br />
<strong>缺点：</strong></p>
<ul>
<li>对于小型数据集，预训练效果不如传统 CNN。<br />
</li>
<li>模型收敛速度慢。</li>
</ul>
<p><strong>方法：</strong>扩展
BEiT，引入多模态训练框架，通过视觉和文本联合建模，提升图像和语言任务的性能。<br />
<strong>效果：</strong>COCO Captioning-mAP:
58.4%，在多模态任务上优于同类方法。<br />
<strong>缺点：</strong></p>
<ul>
<li>模型复杂度显著增加，训练时间较长。<br />
</li>
<li>对文本和图像的标注数据需求较大。</li>
</ul>
<h3 id="yolov1015-2024">YOLOv10[15]-2024</h3>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125720227.png"
alt="image-20250113125720227" />
<figcaption aria-hidden="true">image-20250113125720227</figcaption>
</figure>
<p><strong>方法：</strong>提出 YOLOv10，进一步优化 YOLO
系列，通过引入动态感受野和自适应特征融合模块，提升检测速度与精度。<br />
<strong>效果：</strong>VOC07-mAP: 75.3%，实时性能提升，推理速度可达 200
fps。<br />
<strong>缺点：</strong></p>
<ul>
<li>对小目标和密集目标的检测仍有改进空间。<br />
</li>
<li>较早的硬件设备可能不支持优化的动态模块。</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/SchwertLin/Pic@main/img/image-20250113125738647.png"
alt="image-20250113125738647" />
<figcaption aria-hidden="true">image-20250113125738647</figcaption>
</figure>
<h3 id="指标">指标</h3>
<p>mAP:综合衡量检测结果</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 43%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>相关,正类（positive）</th>
<th>无关,负类（negative）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>被检索到</td>
<td>true positives（本身是P，预测为P）</td>
<td>false positives（本身是N，预测为P）</td>
</tr>
<tr class="even">
<td>未被检索到</td>
<td>false negatives（本身是P，预测为N）</td>
<td>false negatives（本身是N，预测为N）</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\text{Precision}=\frac{\text{TP}}{\text{TP+FP}}\\
\text{Recall}=\frac{\text{TP}}{\text{TP+FN}}
\]</span></p>
<blockquote>
<p>对Recall的理解：分母是TP+FN，FN表示的是我所遗漏的应该被预测为正例的样本，实际上我把它当作负例了。</p>
</blockquote>
<p>IoU：交并比</p>
<figure>
<img
src="https://i-blog.csdnimg.cn/blog_migrate/0e833fe1f3bf246b8c44e76e8305740f.png"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="想法--凹凸餐盘的菜品识别">想法--凹凸餐盘的菜品识别</h2>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>复杂的餐盘形状</th>
<th>食物混合问题</th>
<th>光线及环境因素</th>
<th>特殊场景需求</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>餐盘的凹凸结构</strong>：凹槽区域、凸起边缘容易遮挡或变形食物轮廓，增加了目标分割的难度。</td>
<td><strong>混合食物的难辨性</strong>：如拼盘菜，或者盛菜过多，食物之间界限模糊，影响识别效果。</td>
<td><strong>光线反射问题</strong>：光线在餐盘表面反射可能导致摄像头捕捉到的图像曝光过度或产生光斑，干扰图像质量。</td>
<td><strong>液体及半流质食品</strong>：如汤类、酱汁等在凹凸餐盘中容易流动，识别时区域划分困难。</td>
</tr>
<tr class="even">
<td><strong>餐盘边界的干扰</strong>：传统的图像分割算法容易将餐盘的边缘误判为目标区域，导致结果不准确。</td>
<td>---</td>
<td><strong>拍摄角度的偏差</strong>：在不同角度拍摄的餐盘可能出现严重的透视畸变，影响识别模型的稳定性。</td>
<td><strong>污渍及残渣干扰</strong>：餐盘表面的食物残渣或污渍容易与目标食物混淆。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Q:若是目标检测：</p>
<ol type="1">
<li>那么我解决这个问题是使用one-stage/two-stage?</li>
<li>对于识别问题的速度要求有多快？</li>
<li>如何在时间和准确率上进行权衡？</li>
</ol>
</blockquote>
<p>现在对于two-stage的似乎可能过时了，因为并不适用于实时的物体检测。</p>
<p>现在还没看One-stage的详细步骤，但是我揣测是可以适用YOLO
v8/10来进行。</p>
<p>问题：应用问题。应用难点是：凹凸餐盘造成的前后景难以辨析，影响了检测识别的准确率。</p>
<p><strong>这个写得也太简单了、对问题的认识不够深入。</strong></p>
<h2 id="引用文献">引用文献</h2>
<p>[1] Girshick, Ross, et al. “Rich Feature Hierarchies for Accurate
Object Detection and Semantic Segmentation.” 2014 IEEE Conference on
Computer Vision and Pattern Recognition, 2014,
https://doi.org/10.1109/cvpr.2014.81.</p>
<p>[2] He, Kaiming, et al. “Spatial Pyramid Pooling in Deep
Convolutional Networks for Visual Recognition.” IEEE Transactions on
Pattern Analysis and Machine Intelligence, Sept. 2015, pp. 1904–16,
https://doi.org/10.1109/tpami.2015.2389824.</p>
<p>[3] Girshick, Ross. “Fast R-CNN.” 2015 IEEE International Conference
on Computer Vision (ICCV), 2015,
https://doi.org/10.1109/iccv.2015.169.</p>
<p>[4] Ren, Shaoqing, et al. “Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks.” IEEE Transactions on Pattern
Analysis and Machine Intelligence, June 2017, pp. 1137–49,
https://doi.org/10.1109/tpami.2016.2577031.</p>
<p>[5] Lin, Tsung-Yi, et al. “Feature Pyramid Networks for Object
Detection.” 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017, https://doi.org/10.1109/cvpr.2017.106.</p>
<p>[6] He, Kaiming, et al. "Mask r-cnn." <em>Proceedings of the IEEE
international conference on computer vision</em>. 2017.</p>
<p>[7] Redmon, Joseph, et al. “You Only Look Once: Unified, Real-Time
Object Detection.” 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, https://doi.org/10.1109/cvpr.2016.91.</p>
<p>[8] Lin, Tsung-Yi, et al. “Focal Loss for Dense Object Detection.”
2017 IEEE International Conference on Computer Vision (ICCV), 2017,
https://doi.org/10.1109/iccv.2017.324.</p>
<p>[9] Tan, Mingxing, Ruoming Pang, and Quoc V. Le. "Efficientdet:
Scalable and efficient object detection." <em>Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition</em>.
2020.</p>
<p>[10] Liu, Ze, et al. "Swin transformer: Hierarchical vision
transformer using shifted windows." <em>Proceedings of the IEEE/CVF
international conference on computer vision</em>. 2021.</p>
<p>[11] Xu, Yufei, et al. "Vitae: Vision transformer advanced by
exploring intrinsic inductive bias." <em>Advances in neural information
processing systems</em> 34 (2021): 28522-28535.</p>
<p>[12] Zhang, Hao, et al. "Dino: Detr with improved denoising anchor
boxes for end-to-end object detection." <em>arXiv preprint
arXiv:2203.03605</em> (2022).</p>
<p>[13] Bao, Hangbo, et al. "Beit: Bert pre-training of image
transformers." <em>arXiv preprint arXiv:2106.08254</em> (2021).</p>
<p>[14] Wang, Wenhui, et al. "Image as a foreign language: Beit
pretraining for vision and vision-language tasks." <em>Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
2023.</p>
<p>[15] Wang, Ao, et al. "Yolov10: Real-time end-to-end object
detection." <em>arXiv preprint arXiv:2405.14458</em> (2024).</p>
<h1 id="软件工程大作业">软件工程大作业</h1>
<p>感觉二因为缺少企业的数据，但是能不能去爬一点数据？</p>
<p>感觉可以结合LLM的方向：</p>
<ul>
<li>智能职位匹配(但是集合AI感觉就行，没必要做LLM)</li>
<li>辅助求职:准备一些题库，去除原先的评分标准，现在让求职者回答一些问题，然后模型打分、并给出一些求职建议或经验。</li>
</ul>
<p>设置有关的prompt：比如，假设你是一个求职专家，你需要给予不同专业方向的面试者一些题（？）</p>
<p>对于企业方，可以查看所有应聘的人，但是旁边会标注上此项得分，帮助HR进行简历初筛。</p>
<p><strong>Q:什么问题？</strong></p>
<p>首先，求职者分很多的类别，我们可以先设计一些疑问。然后对方求职者回答采用文字陈述的方式输出。</p>
<p>再将这些文字陈述+简历信息投入LLM，获得对此人的初步评价以及得分。</p>
</div><div class="post-end"><div class="post-prev"><a href="/2025/01/11/%E6%96%B0%E6%A0%87%E6%97%A5%E5%88%9D%E4%B8%8A-%E7%AC%AC1%E5%8D%95%E5%85%83/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2025/01/10/%E9%AB%98%E7%BA%A7%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B-%E5%A4%8D%E4%B9%A0-%E4%B9%A0%E9%A2%98%E5%9B%9E%E5%BF%86/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#object-detection"><span class="toc-content-number">1.</span> <span class="toc-content-text">Object Detection</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#two-stage"><span class="toc-content-number">1.1.</span> <span class="toc-content-text">Two-Stage</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#rcnn1-2014"><span class="toc-content-number">1.1.1.</span> <span class="toc-content-text">RCNN[1]-2014</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#sppnet2-2014"><span class="toc-content-number">1.1.2.</span> <span class="toc-content-text">SPPNet[2]-2014</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#fast-rcnn3-2015"><span class="toc-content-number">1.1.3.</span> <span class="toc-content-text">Fast RCNN[3]-2015</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#faster-rcnn4-2015"><span class="toc-content-number">1.1.4.</span> <span class="toc-content-text">Faster RCNN[4]-2015</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#fpn5-2017"><span class="toc-content-number">1.1.5.</span> <span class="toc-content-text">FPN[5]-2017</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#mask-rcnn6-2017"><span class="toc-content-number">1.1.6.</span> <span class="toc-content-text">Mask-RCNN[6]-2017</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#one-stage"><span class="toc-content-number">1.2.</span> <span class="toc-content-text">One-Stage</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#yolo7-2016"><span class="toc-content-number">1.2.1.</span> <span class="toc-content-text">YOLO[7]-2016</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#retinanet8-2017"><span class="toc-content-number">1.2.2.</span> <span class="toc-content-text">RetinaNet[8]-2017</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#efficientdet9-2020"><span class="toc-content-number">1.2.3.</span> <span class="toc-content-text">EfficientDet[9]-2020</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#swin-transformer10-2021"><span class="toc-content-number">1.2.4.</span> <span class="toc-content-text">Swin Transformer[10]-2021</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#vitae11-2021"><span class="toc-content-number">1.2.5.</span> <span class="toc-content-text">ViTAE[11]-2021</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#dino12-2022"><span class="toc-content-number">1.2.6.</span> <span class="toc-content-text">DINO[12]-2022</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#beit1314-20212023"><span class="toc-content-number">1.2.7.</span> <span class="toc-content-text">BEiT[13,14]-2021&#x2F;2023</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#yolov1015-2024"><span class="toc-content-number">1.2.8.</span> <span class="toc-content-text">YOLOv10[15]-2024</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#%E6%8C%87%E6%A0%87"><span class="toc-content-number">1.2.9.</span> <span class="toc-content-text">指标</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E6%83%B3%E6%B3%95--%E5%87%B9%E5%87%B8%E9%A4%90%E7%9B%98%E7%9A%84%E8%8F%9C%E5%93%81%E8%AF%86%E5%88%AB"><span class="toc-content-number">1.3.</span> <span class="toc-content-text">想法--凹凸餐盘的菜品识别</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE"><span class="toc-content-number">1.4.</span> <span class="toc-content-text">引用文献</span></a></li></ol></li><li class="toc-content-item toc-content-level-1"><a class="toc-content-link" href="#%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%A4%A7%E4%BD%9C%E4%B8%9A"><span class="toc-content-number">2.</span> <span class="toc-content-text">软件工程大作业</span></a></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>