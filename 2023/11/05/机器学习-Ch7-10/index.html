<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="主要内容：K-means、GMM、Kernel Method、集成学习、CNN Contents [TOC] Ch7 Clustering Unsupervised learning Learning without supervision  Find the distribution of data Learning to generate samples Clustering">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-Ch7-10">
<meta property="og:url" content="http://example.com/2023/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Ch7-10/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="主要内容：K-means、GMM、Kernel Method、集成学习、CNN Contents [TOC] Ch7 Clustering Unsupervised learning Learning without supervision  Find the distribution of data Learning to generate samples Clustering">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-11-05T08:09:39.000Z">
<meta property="article:modified_time" content="2025-01-05T08:15:20.643Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary"><title>机器学习-Ch7-10 - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="主页">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>归档</span></a><a class="top-menu-item" href="/categories"><span>分类</span></a><a class="top-menu-item" href="/tags"><span>标签</span></a><a class="top-menu-item" href="/about"><span>关于</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="搜索"><i class="icon fa-solid fa-magnifying-glass"></i><span>搜索</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="打开菜单"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>归档</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>分类</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>标签</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>关于</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>机器学习-Ch7-10</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="发布日期"></i><time class="publish-time">2023-11-05</time><i class="icon fa-regular fa-calendar-check" title="更新日期"></i><time class="update-time">2025-01-05</time></div>

<div class="post-tags"><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><i class="icon fa-solid fa-tags" title="标签"></i><a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="版权声明"></i><span>版权声明: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">署名-非商业性使用-禁止演绎 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>全文约3.4K字</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">阅读次数: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><p>主要内容：<code>K-means</code>、<code>GMM</code>、<code>Kernel Method</code>、<code>集成学习</code>、<code>CNN</code></p>
<h2 id="contents">Contents</h2>
<p>[TOC]</p>
<h2 id="ch7-clustering">Ch7 Clustering</h2>
<h3 id="unsupervised-learning">Unsupervised learning</h3>
<p>Learning without supervision</p>
<ul>
<li>Find the distribution of data</li>
<li>Learning to generate samples</li>
<li>Clustering</li>
<li>Anomaly detection</li>
<li>Feature learning</li>
</ul>
<p>Clustering</p>
<ul>
<li>One of the most important unsupervised learning tasks</li>
<li>Clustering is the process of identifying groups, or clusters, of
data points in a (usually) multidimensional space.</li>
<li>Connection to distribution modeling: related to mixture model</li>
</ul>
<p>Clustering: discover groups such that samples within a group are more
similar to each other than samples across groups.</p>
<p>Clustering的Application：</p>
<ul>
<li>Segmentation</li>
<li>Superpixel</li>
<li>Vector quantization in Bag-of-feature model</li>
</ul>
<p>Clustering的Ingredient:</p>
<ul>
<li>A dissimilarity function between samples</li>
<li>A loss function to evaluate clusters</li>
<li>Algorithm that optimizes this loss function</li>
</ul>
<p>衡量dissimilarity function: <span class="math display">\[
D(x_i,X_i&#39;)=\sqrt{\sum^p_{i=1}(x_{ij}-x_{i&#39;j})^2}
\]</span> <span class="math inline">\(x_{ij}\)</span>: <span
class="math inline">\(x_i\)</span>的特征点，j=1,2,<span
class="math inline">\(\dots\)</span>,p</p>
<p>Clustering is invariant to rotation and translation of features,
<strong>but not to scaling</strong>.</p>
<h3 id="k-means-clustering">K-means Clustering</h3>
<p>Basic idea:</p>
<ul>
<li>Each sample can only fall into one of the k groups</li>
<li>From each group, we can calculate the mean vectors, that is, the
average of samples falling into a group</li>
<li>Any sample should be closest to the mean vector of its own group
than the mean vectors of other groups</li>
</ul>
<p>K-means take an <strong>iterative</strong> approach for solving the
problem.</p>
<p><strong>Algorithm</strong>:</p>
<ul>
<li>E-step: fixed the current mean vectors of each group. If a sample is
closest to the i-th mean vector, then the sample is assigned to the i-th
group</li>
<li>M-step: fixed the assignment, calculate the mean vector of each
group</li>
<li>Iterate E step and M step, until converge</li>
</ul>
<p><strong>Questions</strong>:</p>
<p>Why using those two steps can lead to converged result?</p>
<blockquote>
<p>通过反复迭代E-steps和M-steps,
K-means算法会逐渐优化簇分配和质心的位置，直到达到收敛状态。在收敛状态下，簇分配不再发生变化，质心的位置也相对稳定。这意味着算法已经找到了一个相对稳定的聚类结果，达到了收敛状态。</p>
</blockquote>
<p>Why always calculate mean vectors?</p>
<blockquote>
<p>为了更新簇的位置，并在每次迭代中重新计算数据点与质心之间的距离，从而进行簇分配。</p>
</blockquote>
<p>What is the objective function of k-means clustering algorithm?</p>
<p>– Objective function will give a measurement of the goodness of
clustering results</p>
<blockquote>
<p><span class="math display">\[
\mathcal{J} = \sum_{i=1}^{n} \sum_{i=1}^K r_{ik} ||x_i - μ_k||^2_2
\]</span></p>
<p><span class="math inline">\(r_{ik}\)</span>: Each point will be
assigned to one of the prototypes, the distance between the point to the
prototype is the cost of the assignment. We are seeking the optimal
assignment that can minimize the cost.</p>
<p><span class="math inline">\(\mu_k\)</span>: We are also seeking the
optimal prototypes that can minimize the total cost.</p>
</blockquote>
<p>Choose K? <strong>Cross validation</strong></p>
<p><strong>Limitations of k-means</strong></p>
<ol type="1">
<li>Hard assignments of data points to clusters can cause a small
perturbation to a data point to flip it to another cluster</li>
<li>Assumes spherical clusters and equal probabilities for each
cluster</li>
<li>Those limitations can be solved by GMM based clustering</li>
</ol>
<h3 id="gaussian-mixture-modelsgmm">Gaussian Mixture Models(GMM)</h3>
<p>混合高斯模型：GMM can characterize the distribution that contains K
clusters.
GMM假设观察的数据点是从多个高斯分布的混合中生成的，每一个高斯分布有一个<span
class="math inline">\(\pi_k\)</span>(weight)，表示选择这种分布的可能性。
<span class="math display">\[
多元高斯分布：P(x)=\mathcal{N}(x|\mu_k,\Sigma_k)=\frac{exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1})(x-\mu)}{\sqrt{(2\pi)^k|\Sigma|}}\\
高斯混合模型：P(x)=\sum^K_{k=1}\pi_k\mathcal{N}(x|\mu_k,\Sigma_k),\sum^K_{k=1}\pi_k=1,0\le\pi_k\le1
\]</span> Generative process:</p>
<ol type="1">
<li><p>Randomly select the k-th Gaussian distribution according to <span
class="math inline">\(\pi_k\)</span></p></li>
<li><p>Randomly sample x from the selected Gaussian</p>
<p>distribution</p></li>
</ol>
<p><span class="math display">\[
P(\gamma=k)=\pi_k\\
P(x|\gamma=k)=\mathcal{N}(x|\mu_k,\sum_k)\\
P(x)=\sum^K_{k=1}\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span></p>
<p><span class="math inline">\(\gamma\)</span>: latent variable,
用于从多个高斯分布中选择特定的高斯分布。<span
class="math inline">\(\gamma\)</span>根据相关概率（<span
class="math inline">\(\pi_k\)</span>）去选择哪个高斯分布去生成数据点。</p>
<p><strong>Latent Variable</strong>：</p>
<ol type="1">
<li>Intermediate results inside a generative process</li>
<li>Each sample is associated with a latent variable</li>
<li>We do not know its exact value</li>
<li>But we can infer how likely it can be</li>
</ol>
<p>对于隐变量，我们无法直接观察或测量它，但是它对最终的结果有影响。只能推测其可能性。</p>
<blockquote>
<p>比如学生的学习能力，是latent
variable，会对考试结果产生影响，但是我们不能确切地衡量学生地学习能力。</p>
</blockquote>
<p>parameter estimation for GMM: <strong>use
MLE(最大似然估计)，然后求解偏导<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
\theta}=0\)</span></strong></p>
<p>EM algorithm solution to GMM</p>
<ul>
<li><p>E-step: calculate the posterior probability about the latent
variable (for each <span class="math inline">\(x_i\)</span> calc prob
<span class="math inline">\(r’_{ik}\)</span> for each cluster, this
corresponds to assigning <span class="math inline">\(x_i\)</span> to a
cluster in k-means) <span class="math display">\[
r&#39;_{ik}=P(r_{ik}=1|x_i)=\frac{\mathcal{N}(x_i|\mu_k,\Sigma_K)\pi_k}{\sum^K_{j=1}\mathcal{N}(x_i|\mu_j,\Sigma_j)\pi_j}
\]</span></p></li>
<li><p>M-step: estimate parameters based the expectation of latent
variables (recalc GMM based on estimate in step E, in k-means this is
recalc the centroids) <span class="math display">\[
\pi_k=\frac{\sum_i r&#39;_{ik}}{N}\\
\mu_k=\frac{\sum_ir&#39;_{ik}x_i}{\sum_ir&#39;_{ik}}\\
\Sigma_K=\frac{\sum_ir&#39;_{ik}(x_i-\mu_k)x_i-\mu_k)^T}{\sum_ir&#39;_{ik}}
\]</span></p></li>
</ul>
<p>EM algorithm (More general case)</p>
<ol type="1">
<li>The above solution represents a special case of a more general
method called EM-algorithm</li>
<li>It is widely used for learning the probabilistic models with latent
variable inside its generative process.</li>
<li>It iterates between an E-step and an M-step.<br />
</li>
<li>We can theoretically prove that each step will lead to a nonincrease
of loss function</li>
<li>The iterative process will converge to a local minimum</li>
</ol>
<p>其通用形式如下:</p>
<ul>
<li><p>E-step:计算隐变量的后验概率:<span
class="math inline">\(P(z|x,\theta^t)\)</span>, <span
class="math inline">\(\theta^t\)</span>是目前模型的参数</p></li>
<li><p>M-step:基于期望的似然估计更新模型参数 <span
class="math display">\[
\theta^{t+1}=\mathop{\arg\max}\limits_{\theta}E_{z|x,\theta}\log
P(z,x|\theta)=\int P(z|x,\theta^t)\log(P(x|\theta,z)P(z|theta))dz
\]</span></p></li>
</ul>
<p><strong>与K-means 的 connection</strong>：</p>
<ul>
<li>E-step in GMM is a soft version of K-means. <span
class="math inline">\(r_{ik}\in[0,1]\)</span>, instead of <span
class="math inline">\(\{0,1\}\)</span>.</li>
<li>M-step in GMM estimates the probabilities and the covariance matrix
of each cluster in addition to the means.</li>
<li>All <span class="math inline">\(\pi_k\)</span> are equal. <span
class="math inline">\(\sum_k=\delta^2I\)</span>, as <span
class="math inline">\(\delta^2\rightarrow0,r_{ik}\rightarrow\{0,1\}\)</span>,
and the 2 methods coincide.</li>
</ul>
<h2 id="ch8-kernel-method">Ch8 Kernel Method</h2>
<p>Kernel function的性质（证明）：</p>
<ul>
<li>positive semi-definite(半正定)：<span
class="math inline">\(c^Tkc\ge0\)</span></li>
</ul>
<h4 id="polynomial-kernel">Polynomial Kernel</h4>
<p>多次项核函数，比较常用。其形式如下： <span class="math display">\[
K(x_i,x_j)=(x_i\cdot x_j+c)^d
\]</span></p>
<h4 id="radial-basis-function-kernel高斯核">Radial Basis Function
Kernel(高斯核)</h4>
<p>更常用的核函数，其形式如下： <span class="math display">\[
K(x_i,x_j)=e^{-\gamma(x_i-x_j)^2}\\
=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}
\]</span></p>
<h3 id="kernel-in-svm">Kernel in SVM</h3>
<p>我们原始的对偶问题是： <span class="math display">\[
\mathcal{L}=\sum\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j
\]</span> 新的对偶问题实际上是把<span
class="math inline">\(x\)</span>换成了<span
class="math inline">\(\phi(x)\)</span>(升维): <span
class="math display">\[
\mathcal{L}=\sum\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_j\phi(x_i)\phi(x_j)
\]</span>
我们选定核函数：<em>原先的升维函数不宜求解，但核函数可以求解</em> <span
class="math display">\[
k(x_i,x_j)=\phi(x_i)\cdot\phi(x_j)
\]</span></p>
<blockquote>
<ul>
<li>We stop caring about what the transform represents</li>
<li>We start caring about how well it classifies our data</li>
</ul>
</blockquote>
<p>其他详细地推导没有，主要是在CVX中的应用。</p>
<h3 id="kernel-in-k-means">Kernel in K-means</h3>
<p>Map the data space into some kernel space.</p>
<p>Hope this new space better allows the data to be separated!</p>
<p>下面是K-means的过程：</p>
<p>1.Assigning Points to Clusters</p>
<p>(原来的) <span class="math display">\[
assign(x_i)=\mathop{\arg\min}\limits_{k}||x_i-\mu_k||^2\\
=(x_i-\mu_k)^T(x_i-\mu_k)
\]</span> （将<span class="math inline">\(x_i\)</span>换成了<span
class="math inline">\(\phi(x_i)\)</span>的） <span
class="math display">\[
assign(x_i)=\mathop{\arg\min}\limits_{k}||\phi(x_i)-\mu_k||^2\\
=(\phi(x_i)-\mu_k)^T(\phi(x_i)-\mu_k)
\]</span> 2.Defining Cluster Mid-Points</p>
<p>(原来的) <span class="math display">\[
\mu_k=\frac 1 {N_k}\sum^N_{i=1}z_{ik}x_i
\]</span> <span class="math inline">\(Z_{ik}=1\)</span>：样本<span
class="math inline">\(x_i\)</span>属于k类，=0不属于k类</p>
<p>（将<span class="math inline">\(x_i\)</span>换成了<span
class="math inline">\(\phi(x_i)\)</span>的） <span
class="math display">\[
\mu_k=\frac {\sum_{i=1}z_{ik}\phi(x_i)}{\sum_{i=1}z_{ik}}
\]</span></p>
<h3 id="kernel-in-linear-regression">Kernel in Linear Regression</h3>
<p>原来的： <span class="math display">\[
\sum_i||w^Tx_i-y_i||^2
\]</span> 现在的： <span class="math display">\[
\sum_i||w^T\phi(x_i)-y_i||^2
\]</span> 我们令<span
class="math inline">\(w=\sum_j\alpha_jx_j+o\)</span>,代入目标函数得：
<span class="math display">\[
\sum_i||(\sum_j\alpha_jx_j+o)^Tx_i-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jx_i\cdot x_j+x_i\cdot o-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jK(x_i,x_j)+K(x_i,o)-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jK(x_i,x_j)-y_i||^2_2
\]</span></p>
<blockquote>
<p>o: 映射函数φ(x)可能带入的一个额外 bias term</p>
<p><span class="math inline">\(K(x_i,o)=0\)</span>:</p>
<ol type="1">
<li>o不代表任何一个实际样本点。它仅仅表示引入核技巧可能带来的一个额外的bias项。</li>
<li>因此,任何样本点<span
class="math inline">\(x_i\)</span>与o的距离<span
class="math inline">\(||x_i -
o||\)</span>都是无定义的。无法直接计算核函数值。</li>
<li>因此就直接定义=0</li>
</ol>
</blockquote>
<h3 id="kernel-in-pca">Kernel in PCA</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>Tasks</th>
<th>Problems</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Replace x with <span class="math inline">\(\phi(x)\)</span></td>
<td>Centralising the data</td>
</tr>
<tr class="even">
<td>Turn everything into dot products</td>
<td>Calculating the covariance matrix</td>
</tr>
<tr class="odd">
<td>Use <span class="math inline">\(K(x_i , x_j )\)</span> a lot</td>
<td>Calculating the projection matrix</td>
</tr>
<tr class="even">
<td>Never use <span class="math inline">\(\phi(x)\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
Cov=X^TX\\
Cov(k,m)=\sum_i^nX_i^kX^m_i\\
=\sum^n_{i,j=1}c_ic_jk(x_ix_j)=c^TKc\ge0
\]</span></p>
<p><strong>计算协方差矩阵</strong>： <span class="math display">\[
\bar X\bar X^Tv=\lambda v\\
\]</span></p>
<blockquote>
<p><span class="math inline">\(\bar X\)</span>: centralised data
matrix(d x n)</p>
<p>d: 属性的维数，n：有多少样本</p>
<p><span class="math inline">\(\bar X\bar X^T\)</span>:(d x
d)协方差矩阵</p>
<p><span class="math inline">\(v\)</span>: 协方差矩阵的特征向量</p>
</blockquote>
<p>经过kernel matrix: <span class="math display">\[
\bar X\bar X^Tv&#39;=\lambda v&#39;\\
v=\bar Xv&#39;
\]</span> <strong>计算去中心化</strong>： <span class="math display">\[
\bar k(x_i,x_j)=(\varphi(x_i)-\mu)^T(c-\mu)\\
=(\varphi(x_i)-\frac 1 N\sum_k\varphi(x_k))^T(\varphi(x_j)-\frac 1
N\sum_k\varphi(x_k))\\
=\varphi(x_i)\cdot\varphi(x_j)-\frac 1
N\sum_k\varphi(x_i)\cdot\varphi(x_k)-\frac 1
N\sum_k\varphi(x_j)\cdot\varphi(x_k)-\frac 1
{N^2}\sum_k\varphi(x_i)\cdot\varphi(x_j)\\
=k(x_i,x_j)-\frac 1 N \sum_kk(x_i,x_k)-\frac 1 N \sum_kk(x_j,x_k)-\frac
1 {N^2} \sum_{ij}k(x_i,x_j)\\
K&#39;=K-1_NK-k1_N+1_NK1_N
\]</span></p>
<blockquote>
<p><span class="math inline">\(1_N\)</span> here represents a NxN matrix
filled with 1/N</p>
</blockquote>
<p><strong>计算投影矩阵</strong>：原来是<span
class="math inline">\(y=P^T(x-\mu)\)</span>,现在是： <span
class="math display">\[
y_k=p_k^T(\varphi(x_t)-\mu)
\]</span> 带入<span class="math inline">\(v=\varphi(\bar
X)v&#39;\)</span>: <span class="math display">\[
y_k=v&#39;^T_k\varphi(\bar X)^T(\varphi(x_t)-\frac 1
N\sum_i\varphi(x_i))\\
=v&#39;^T_k(\varphi(X)-\frac 1 N\sum_i\varphi(x_i))^T(\varphi(x_t)-\frac
1 N\sum_i\varphi(x_i))
\]</span></p>
<h2 id="ch9-ensemble-method">Ch9 Ensemble Method</h2>
<p>When to use?</p>
<ul>
<li>Individual decisions are cheap, so making many of them is easy</li>
<li>Making an expert is expensive or impossible</li>
<li>Your amateurs are independent of each other</li>
</ul>
<h3 id="decision-tree">Decision Tree</h3>
<p>Entropy=<span class="math inline">\(\sum-p_i\log_2p_i\)</span>(Gini
Function), <span class="math inline">\(p_i\)</span>代表属于class i
的概率</p>
<p>Gini Coefficient of
Purity是利用Gini不均衡度来度量分类问题样本集合的纯程度。它服判定模型预测性能的一个指标(衡量决策树每个节点的纯度):</p>
<ul>
<li>它通过计算样本集合中的类分布不均衡程度,来反映纯度程度。</li>
<li>如果样本全是同一类,类分布最均衡,Gini值为0,这也就是100%的纯度。</li>
<li>否则不同类样本混杂,Gini值越高,表示纯度越低。</li>
</ul>
<p><strong>The Flaw of the Decision Tree</strong>:</p>
<ul>
<li>We have a series of decisions, but some of these decisions can only
be conceived of as overfitting.
（根据训练数据集递归地构建依赖于数据的决策边界,不断地细分训练集样本）</li>
<li>There are numerous places we could place decision boundaries.
(会导致决策边界过于复杂，训练集效果不错，但是测试集的效果很差)</li>
<li>How can we avoid these overfit decision
boundaries?--比如剪枝，或限定最大树深</li>
</ul>
<h3 id="random-forest">Random Forest</h3>
<p>定义：A random forest is a series of decision trees which are
randomized. You then apply your new measurement to each decision tree,
picking the aggregate decision by voting.</p>
<p>容易出现的问题：</p>
<ul>
<li>Some potential decision boundaries represent overfitting.</li>
<li>Some features are not representative of generalisation.</li>
</ul>
<p>我们使用bootstrapping方法（随机有放回地采样--random selection with
replacement）得到子数据集（subsets）</p>
<p>值得考虑地问题是：</p>
<p>How many trees?</p>
<ul>
<li>Too few trees:
<ul>
<li>Might interpret random variation as signal.</li>
<li>Biased towards individual trees.</li>
</ul></li>
<li>Too many trees: Slow?</li>
</ul>
<p>How many dimensions/tree? --从实践来看一般是<span
class="math inline">\(\sqrt{D}\)</span>或是<span
class="math inline">\(\log(D)\)</span></p>
<p><strong>Bagging doesn’t choose a random subset of
dimensions</strong></p>
<blockquote>
<ul>
<li>Bagging算法为了减轻过拟合,其主要思想是从原始训练数据集中采样出多个并行的子数据集,来训练多个基学习器模型。(采样过程使用的是boostrapping方法)</li>
<li>但在采样子数据集时,Bagging采样的是样本实例,而不是特征维度。</li>
<li>也就是说,每个子数据集中包含的是原始所有特征维度对应的一部分样本。而不是随机选择一部分特征维度。</li>
</ul>
</blockquote>
<h3 id="boosting">Boosting</h3>
<p>idea: We build a classifier, but it incorrectly classifies some
samples, we can further train this classifier to perform better on the
incorrectly labelled samples(从错误中学习）</p>
<ul>
<li>We make decisions based on a weighted decision of minimal ‘weak
classifiers’ (stumps)</li>
<li>After we have built our ‘one stump’ we can judge how good it is, by
the number of instances it gets wrong.</li>
</ul>
<p><span class="math display">\[
\alpha=\frac1 2\ln(\frac{1-TotalError}{TotalError})
\]</span></p>
<p><span class="math inline">\(\alpha\)</span>代表每个decision tree
在最终的决策中的贡献程度。我们通过它更新权重： <span
class="math display">\[
w_{new}=w_{old}\times e^a
\]</span></p>
<h4 id="adaboost">Adaboost</h4>
<p>目标函数： <span class="math display">\[
\min_{\{a_k,\theta_m\}}\sum_ie^{-y_ih_m(x_i)}\\
h_m(x)=\alpha_1h(x;\theta_1)+\dots+\alpha_mh(x;\theta_m)
\]</span> loss function: <span class="math display">\[
e^{-yh_m(x)}
\]</span></p>
<p>还有其他的boost的方法，比如XGboost，Gradient Boost。</p>
<p>What do we have in common?</p>
<ul>
<li>We are weighting our bad decisions higher (i.e., boosting)</li>
<li>We are using ensemble learning</li>
</ul>
<h2 id="ch10-deep-learning">Ch10 Deep Learning</h2>
<p>关于Loss的介绍：</p>
<ol type="1">
<li>Loss is some measure of the error between the predicted and target
values</li>
<li>A metric for determining how well a model has trained</li>
<li>Lower loss/cost/error =&gt; better model =&gt; better learned
parameters</li>
</ol>
<p>我们常用的Loss是MSE(Mean Squared Error): <span
class="math display">\[
MSE(X,h_{\theta})=\frac1 m\sum^m_{i=1}(\theta^Tx_i-y_i)^2
\]</span></p>
<p>全局最小值以及局部最小值：Global Minimum is the smallest error out of
all possible parameter configurations, Local Minimum is the smallest
error in some region of space.</p>
<p>一般使用梯度下降(Gradient Descent)来更新参数： <span
class="math display">\[
\theta_{n+1}=\theta_n-\eta\nabla_{\theta}MSE(\theta_n)
\]</span></p>
<h3 id="learning-rateeta">Learning Rate(<span
class="math inline">\(\eta\)</span>)</h3>
<p><span class="math inline">\(\eta\)</span> is a Hyperparameter!</p>
<p>Choice of learning rate is very important:</p>
<ul>
<li>Too small… takes forever to train, can easily get stuck in tiny
local minima</li>
<li>Too big… can overstep or become unstable</li>
</ul>
<p><strong>Activation
Function</strong>：常用的有sigmoid，relu，Tanh函数</p>
<p>Softmax:将结果转化为概率分布 <span class="math display">\[
softmax:\frac{exp(s_k(x))}{\sum^K_{i=1}exp(s_i(x))}\\
cost\ function:\mathcal{J}(\theta)=-\frac 1 m
\sum^m_{i=1}\sum^K_{k=1}y_k^i\log(\hat p_k^i)
\]</span></p>
<h3 id="neural-networks">Neural Networks</h3>
<p>Terms:</p>
<ul>
<li>Iterations (one per ‘batch)</li>
<li>Epochs (one per ‘all samples’)</li>
<li>Batches(对于图片来说就是mega-pixels)</li>
</ul>
<h3 id="cnn">CNN</h3>
<p>其步骤如下：</p>
<p>1.卷积层(Convolutional Layer)--利用padding补全</p>
<ul>
<li>使用卷积操作提取图像中的特征,如边缘、色彩等。</li>
<li>参数包括过滤器(kernel)和偏置。</li>
</ul>
<p>2.池化层(Pooling Layer)</p>
<ul>
<li>对特征图区域进行下采样,降低空间维度,增加翻转不变性和控制过拟合。</li>
<li>常用最大池化和平均池化。</li>
</ul>
<p>3.激活层(Activation Layer)</p>
<ul>
<li>为卷积层和全连接层添加非线性机能,如ReLU、Sigmoid、Tanh等。</li>
</ul>
<ol type="1">
<li>全连接层(Fully Connected Layer)</li>
</ol>
<ul>
<li>将上一层特征整合成一维数组,与该层权值相乘实现分类或回归功能。</li>
</ul>
<p>4.Softmax层</p>
<ul>
<li>将全连接层输出进行Softmax归一化,输出概率评分实现分类。</li>
</ul>
</div><div class="post-end"><div class="post-prev"><a href="/2023/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%80%BB%E7%BB%93/" title="上一篇文章"><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2023/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Ch4-6/" title="下一篇文章"><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>目录</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#contents"><span class="toc-content-number">1.</span> <span class="toc-content-text">Contents</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch7-clustering"><span class="toc-content-number">2.</span> <span class="toc-content-text">Ch7 Clustering</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#unsupervised-learning"><span class="toc-content-number">2.1.</span> <span class="toc-content-text">Unsupervised learning</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#k-means-clustering"><span class="toc-content-number">2.2.</span> <span class="toc-content-text">K-means Clustering</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#gaussian-mixture-modelsgmm"><span class="toc-content-number">2.3.</span> <span class="toc-content-text">Gaussian Mixture Models(GMM)</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch8-kernel-method"><span class="toc-content-number">3.</span> <span class="toc-content-text">Ch8 Kernel Method</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#polynomial-kernel"><span class="toc-content-number">3.0.1.</span> <span class="toc-content-text">Polynomial Kernel</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#radial-basis-function-kernel%E9%AB%98%E6%96%AF%E6%A0%B8"><span class="toc-content-number">3.0.2.</span> <span class="toc-content-text">Radial Basis Function
Kernel(高斯核)</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-svm"><span class="toc-content-number">3.1.</span> <span class="toc-content-text">Kernel in SVM</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-k-means"><span class="toc-content-number">3.2.</span> <span class="toc-content-text">Kernel in K-means</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-linear-regression"><span class="toc-content-number">3.3.</span> <span class="toc-content-text">Kernel in Linear Regression</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-pca"><span class="toc-content-number">3.4.</span> <span class="toc-content-text">Kernel in PCA</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch9-ensemble-method"><span class="toc-content-number">4.</span> <span class="toc-content-text">Ch9 Ensemble Method</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#decision-tree"><span class="toc-content-number">4.1.</span> <span class="toc-content-text">Decision Tree</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#random-forest"><span class="toc-content-number">4.2.</span> <span class="toc-content-text">Random Forest</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#boosting"><span class="toc-content-number">4.3.</span> <span class="toc-content-text">Boosting</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#adaboost"><span class="toc-content-number">4.3.1.</span> <span class="toc-content-text">Adaboost</span></a></li></ol></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch10-deep-learning"><span class="toc-content-number">5.</span> <span class="toc-content-text">Ch10 Deep Learning</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#learning-rateeta"><span class="toc-content-number">5.1.</span> <span class="toc-content-text">Learning Rate(\(\eta\))</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#neural-networks"><span class="toc-content-number">5.2.</span> <span class="toc-content-text">Neural Networks</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#cnn"><span class="toc-content-number">5.3.</span> <span class="toc-content-text">CNN</span></a></li></ol></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="设置"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="目录"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="评论"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="返回顶部"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="深色模式"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="放大字体"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="缩小字体"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>搜索</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="请输入需要搜索的内容……" value=""/></div><div class="search-body"><div id="search-count">匹配结果数: </div><div id="search-result"></div><div id="search-result-empty">未搜索到匹配的文章。</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">总访问量 : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">总访客数 : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously 同步修改评论区主题
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>