<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="author" content="Schwertlilien"/><meta name="keyword"/><meta name="description" content="Contents [TOC] Ch1 Overview of ML å¯¹MLçš„åˆ†ç±»ï¼š ç›‘ç£å­¦ä¹ (Supervised learning) å…¶åŸºæœ¬æ€æƒ³æ˜¯ç»™å®šæ•°æ®é›†ä¸­çš„æ ·æœ¬æ˜¯å¸¦æœ‰&quot;æ­£ç¡®ç­”æ¡ˆ&quot;çš„,å­¦ä¹ è¿™äº›æ•°æ®ä¹‹å,å†æ¥æ–°æ ·æœ¬æ—¶,å¯ä»¥åšå‡ºé¢„æµ‹. å¸¸è§çš„é—®é¢˜æœ‰åƒåœ¾é‚®ä»¶åˆ†ç±»ã€‚ æ— ç›‘ç£å­¦ä¹ (Unsupervised learning) ç»™å®šæ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ã€‚å¸¸ç”¨ç®—æ³•ï¼šèšç±»ã€‚å¸¸è§é—®é¢˜ï¼šæ–°é—»åˆ†ç±»ã€ç»†åˆ†å¸‚">
<meta property="og:type" content="article">
<meta property="og:title" content="æœºå™¨å­¦ä¹ -æ€»ç»“">
<meta property="og:url" content="http://example.com/2023/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="Schwertlilien">
<meta property="og:description" content="Contents [TOC] Ch1 Overview of ML å¯¹MLçš„åˆ†ç±»ï¼š ç›‘ç£å­¦ä¹ (Supervised learning) å…¶åŸºæœ¬æ€æƒ³æ˜¯ç»™å®šæ•°æ®é›†ä¸­çš„æ ·æœ¬æ˜¯å¸¦æœ‰&quot;æ­£ç¡®ç­”æ¡ˆ&quot;çš„,å­¦ä¹ è¿™äº›æ•°æ®ä¹‹å,å†æ¥æ–°æ ·æœ¬æ—¶,å¯ä»¥åšå‡ºé¢„æµ‹. å¸¸è§çš„é—®é¢˜æœ‰åƒåœ¾é‚®ä»¶åˆ†ç±»ã€‚ æ— ç›‘ç£å­¦ä¹ (Unsupervised learning) ç»™å®šæ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ã€‚å¸¸ç”¨ç®—æ³•ï¼šèšç±»ã€‚å¸¸è§é—®é¢˜ï¼šæ–°é—»åˆ†ç±»ã€ç»†åˆ†å¸‚">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-11-05T08:14:58.000Z">
<meta property="article:modified_time" content="2025-01-05T08:16:36.294Z">
<meta property="article:author" content="Schwertlilien">
<meta property="article:tag" content="ç¬”è®°">
<meta property="article:tag" content="æœºå™¨å­¦ä¹ ">
<meta name="twitter:card" content="summary"><title>æœºå™¨å­¦ä¹ -æ€»ç»“ - Schwertlilien - -----personal blog-----</title><link rel="shortcut icon" href="/img/site-icon.png">
<link rel="stylesheet" href="/css/style.css" id="dm-light">


<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css">

<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="top-nav" ondblclick="scrollToTop()"><div class="nav-info"><div class="nav-icon"><img id="nav-icon" src="/img/site-icon.png"/></div><div class="nav-title"><a id="nav-title" href="/" title="ä¸»é¡µ">Schwertlilien</a></div></div><div class="nav-ribbon"><div class="top-menu-expanded"><a class="top-menu-item" href="/archives"><span>å½’æ¡£</span></a><a class="top-menu-item" href="/categories"><span>åˆ†ç±»</span></a><a class="top-menu-item" href="/tags"><span>æ ‡ç­¾</span></a><a class="top-menu-item" href="/about"><span>å…³äº</span></a></div><div class="top-search" onclick="toggleSearchWindow()"><div id="top-search-btn" title="æœç´¢"><i class="icon fa-solid fa-magnifying-glass"></i><span>æœç´¢</span></div></div><div id="top-menu-btn" onclick="openTopMenu()" title="æ‰“å¼€èœå•"><i class="fa-solid fa-bars fa-lg"></i></div></div></div></header><div id="top-menu-hidden"><div class="menu-hidden-content"><div class="menu-hidden-nav"><a class="menu-hidden-item" href="/archives"><i class="fa-solid fa-box-archive fa-sm"></i><span>å½’æ¡£</span></a><a class="menu-hidden-item" href="/categories"><i class="fa-regular fa-folder-open fa-sm"></i><span>åˆ†ç±»</span></a><a class="menu-hidden-item" href="/tags"><i class="fa-solid fa-tags fa-sm"></i><span>æ ‡ç­¾</span></a><a class="menu-hidden-item" href="/about"><i class="fa-solid fa-paw fa-sm"></i><span>å…³äº</span></a></div></div><div class="menu-hidden-blank" onclick="closeTopMenu()"></div></div>
<div class="blog-info"><div class="blog-pic"><img id="blog-pic" src="/img/site-icon.png"/></div><div class="blog-title"><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i><span>Schwertlilien</span><i class="fa-solid fa-paw fa-2xs fa-rotate-by"></i></div><div class="blog-desc">As a recoder: notes and ideas.</div></div><div class="main"><div class="main-content"><article class="post"><div class="post-title"><h1><i class="fa-solid fa-paw"></i>æœºå™¨å­¦ä¹ -æ€»ç»“</h1></div><div class="post-info"><div class="post-info-first-line"><div class="post-date"><i class="icon fa-regular fa-calendar-plus" title="å‘å¸ƒæ—¥æœŸ"></i><time class="publish-time">2023-11-05</time><i class="icon fa-regular fa-calendar-check" title="æ›´æ–°æ—¥æœŸ"></i><time class="update-time">2025-01-05</time></div>

<div class="post-tags"><i class="icon fa-solid fa-tags" title="æ ‡ç­¾"></i><a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">ç¬”è®°</a><i class="icon fa-solid fa-tags" title="æ ‡ç­¾"></i><a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">æœºå™¨å­¦ä¹ </a></div></div><div class="post-info-second-line"><div class="post-copyright"><i class="icon fa-brands fa-creative-commons" title="ç‰ˆæƒå£°æ˜"></i><span>ç‰ˆæƒå£°æ˜: </span><a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" title="CC BY-NC-ND 4.0">ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0</a></div>
<div class="post-word-count"><i class="icon fa-solid fa-pen-to-square"></i><span>å…¨æ–‡çº¦1.1Wå­—</span></div><div class="pageview-post"><i class="icon fa-regular fa-eye"></i><span id="busuanzi_container_page_pv">é˜…è¯»æ¬¡æ•°: <span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner"></i></span></span></div></div></div><div class="post-content"><h2 id="contents">Contents</h2>
<p>[TOC]</p>
<h2 id="ch1-overview-of-ml">Ch1 Overview of ML</h2>
<p>å¯¹MLçš„åˆ†ç±»ï¼š</p>
<p>ç›‘ç£å­¦ä¹ (Supervised learning)
å…¶åŸºæœ¬æ€æƒ³æ˜¯ç»™å®šæ•°æ®é›†ä¸­çš„æ ·æœ¬æ˜¯å¸¦æœ‰"æ­£ç¡®ç­”æ¡ˆ"çš„,å­¦ä¹ è¿™äº›æ•°æ®ä¹‹å,å†æ¥æ–°æ ·æœ¬æ—¶,å¯ä»¥åšå‡ºé¢„æµ‹.
å¸¸è§çš„é—®é¢˜æœ‰åƒåœ¾é‚®ä»¶åˆ†ç±»ã€‚</p>
<p>æ— ç›‘ç£å­¦ä¹ (Unsupervised learning)
ç»™å®šæ•°æ®é›†æ²¡æœ‰æ ‡ç­¾ã€‚å¸¸ç”¨ç®—æ³•ï¼šèšç±»ã€‚å¸¸è§é—®é¢˜ï¼šæ–°é—»åˆ†ç±»ã€ç»†åˆ†å¸‚åœºã€‚</p>
<p><strong>Application:</strong></p>
<ul>
<li>Supervised learning:
<ul>
<li>Linear Classifier</li>
<li>Support Vector Machines (hard SVM, soft SVM)</li>
<li>Kernel Methods</li>
<li>Deep Learning</li>
</ul></li>
<li>Unsupervised learning:
<ul>
<li>Linear Discriminant Analysis(LDA)</li>
<li>Principle Component Analysis (PCA)</li>
<li>Generative Models(e.g. GMM, K-means--clustering)</li>
</ul></li>
</ul>
<h3 id="supervised-learning">Supervised learning</h3>
<p>ä¸€äº›å‚æ•°ä»¥åŠç¬¦å·ï¼š</p>
<p>Model <span class="math inline">\(f\)</span>ï¼Œ Loss Function <span
class="math inline">\(\mathcal{L}\)</span>ï¼Œæ¨¡å‹ä¸­çš„å‚æ•°<span
class="math inline">\(\theta\)</span>ï¼Œè¾“å…¥è¾“å‡º<span
class="math inline">\((x_i,y_i)\)</span></p>
<p>The objective of the supervised learning is to <strong>find the
parameters</strong> that minimises the average loss: <span
class="math display">\[
\min_{\theta}\frac 1 n \sum_{i=1}^n \mathcal{L}(f(x_i;\theta),y_i)
\]</span></p>
<h3 id="unsupervised-learning">Unsupervised learning</h3>
<p>Learning patterns when no specific target output values are
supplied.(å®é™…ä¸Šå°±æ˜¯æ²¡æœ‰label)</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Clustering: group data into groups</li>
<li>Building probabilistic model to explain data</li>
<li>Anomaly detection</li>
</ul>
<p>Types of machine learning: shallow vs. deep
ï¼ˆä¼ ç»Ÿæœºå™¨å­¦ä¹ VSæ·±åº¦å­¦ä¹ ï¼‰</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Traditional machine learning</th>
<th>Deep learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Important step: feature design</td>
<td>Allows raw input</td>
</tr>
<tr class="even">
<td>Usually work with â€œfeature vectorsâ€</td>
<td>End-to-end learning</td>
</tr>
<tr class="odd">
<td>Mapping function is simple, with relatively small number of
parameters</td>
<td>Complex models, with millions of parameters</td>
</tr>
<tr class="even">
<td>Works well if the input can be captured by vectors, small to medium
number of samples</td>
<td>Works well if the â€œright featureâ€ is unknown or the input is complex
and a large number of samples are available</td>
</tr>
</tbody>
</table>
<p>è¿›è¡ŒMLçš„æµç¨‹ï¼š</p>
<ol type="1">
<li>Problem formulation â€“ What is the input? What is the expected
outcome?(è¾“å…¥è¾“å‡ºï¼Ÿ)</li>
<li>Data collection
<ol type="1">
<li>Collect data</li>
<li>Annotationï¼ˆç»™dataä»¥labelï¼‰</li>
</ol></li>
<li>Design machine learning algorithm
<ol type="1">
<li>Choose the machine learning model</li>
<li>Choose the objective function</li>
</ol></li>
<li>Training machine learning model: Learn the decision system from
training data</li>
<li>Applying machine learning model</li>
</ol>
<h3 id="æ•°å­¦åŸºç¡€">æ•°å­¦åŸºç¡€</h3>
<p><span class="math display">\[
\sum^N_{i=1}a_i=\sum_ia_i=\sum_ja_j\\
\sum^M_{i=1}\sum^N_{j=1}a_{ij}=\sum_{i,j}a_{ij}\\
\sum^M_{i=1}\sum^N_{j=1}a_ib_j=(\sum_ia_i)(\sum_jb_j)=(\sum_ia_i)(\sum_ib_i)E
\]</span></p>
<h4 id="çº¿æ€§ä»£æ•°">çº¿æ€§ä»£æ•°</h4>
<p>Iä¸ºå•ä½é˜µã€‚(E)</p>
<p><span
class="math inline">\(\Lambda\)</span>è§†ä¸ºå¯¹è§’çŸ©é˜µï¼Œåªåœ¨å¯¹è§’çº¿ä¸Šçš„å…ƒç´ å¯ä»¥é0ï¼Œå…¶ä»–å…ƒç´ éƒ½æ˜¯0ã€‚</p>
<p>çŸ©é˜µè½¬ç½®ï¼š <span class="math display">\[
(AB)^T=B^TA^T\\
(ABC)^T=C^TB^TA^T
\]</span> é€†çŸ©é˜µinverse: <span class="math display">\[
AA^{-1}=I
\]</span> çŸ©é˜µçš„è¿¹ï¼ˆä¹Ÿå°±æ˜¯å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ç›¸åŠ ï¼Œè¢«ç§°ä¹‹ä¸º<span
class="math inline">\(Tr(A)\)</span>ï¼‰ï¼š <span class="math display">\[
Tr(A)=\sum_ia_{ii},Tr(a)=a\\
Tr(A+B)=Tr(A)+Tr(B)\\
Tr(X^TY)=Tr(XY^T)=Tr(Y^TX)=Tr(YX^T)
\]</span> ä¸¤ä¸ª<strong>å‘é‡</strong>çš„å†…ç§¯ï¼š <span
class="math display">\[
\langle x,y \rangle=x^Ty=\sum_ix_iy_i
\]</span> p-norm: <span class="math display">\[
L1:||x||_2=\sqrt{\sum_ix_i^2}\\
L2:||x||_1=\sum_i|x_i|\\
Lp:||x||_p=(\sum_i|x_i|^p)^{\frac 1 p}
\]</span> æ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼ˆFrobenius normï¼‰ï¼š <span class="math display">\[
||A||_F = \sqrt{ \sum_{ij}a^2_{ij}}=\sqrt{Tr(AA^T)}=\sqrt{Tr(A^TA)}
\]</span></p>
<p>ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼š <span class="math display">\[
Au=\lambda u\\
(A-\lambda I)u=0
\]</span> çŸ©é˜µçš„ç‰¹å¾åˆ†è§£ï¼š <span class="math display">\[
A=Q\Lambda Q^{-1}
\]</span></p>
<blockquote>
<p>A: diagonal
matrixï¼Œå…¶ä¸­é™¤äº†ä¸»å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ï¼Œå…¶ä»–ä½ç½®ä¸Šçš„å…ƒç´ éƒ½ä¸ºé›¶ã€‚å…¶ç¬¬iä¸ªå¯¹è§’å€¼ä¸ºAçš„ç¬¬iä¸ªç‰¹å¾å€¼ã€‚</p>
<p>Q: æ¯ä¸€åˆ—columnæ˜¯ç¬¬iä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚</p>
</blockquote>
<p>å¦‚æœAæ˜¯å¯¹ç§°çš„ï¼ˆsymmetricï¼‰--å¯¹è§’é˜µå¤©ç„¶å¯¹ç§°ï¼š <span
class="math display">\[
A=A^T\\
A=Q\Lambda Q^T\\
Q^TQ=QQ^T=I
\]</span> å…¶ä¸­Qæ»¡è¶³äº†æ­£äº¤çŸ©é˜µï¼ˆOrthogonal Matrixï¼‰çš„æ€§è´¨</p>
<h4 id="çŸ©é˜µæ±‚å¯¼">çŸ©é˜µæ±‚å¯¼</h4>
<p>é¦–å…ˆ<span class="math inline">\(||w||^2_2=w^Tw=\langle
w,w\rangle\)</span> <span class="math display">\[
\frac{\partial Ax}{\partial x}=A\\
\frac{\partial x^TA}{\partial x}=A^T\\
\frac{\partial x^TAx}{\partial x}=Ax+A^Tx\\
\frac{\partial a^Txx^Tb}{\partial x}=ab^Tx+ba^Tx\\
\frac{\partial [f(x)g(x)]}{\partial x}=\frac{\partial f(x)}{\partial
x}g(x)+\frac{\partial g(x)}{\partial x}f(x)
\]</span></p>
<h4 id="æ¦‚ç‡ä¸ç»Ÿè®¡">æ¦‚ç‡ä¸ç»Ÿè®¡</h4>
<p>æœŸæœ›Expectationsï¼š</p>
<p>ç¦»æ•£çš„æƒ…å†µ:<span class="math inline">\(E[X]=\sum_ix_ip_i\)</span></p>
<p>è¿ç»­çš„æƒ…å†µ:<span
class="math inline">\(E[X]=\int_{\mathbb{R}}xf(x)dx\)</span></p>
<p>æ–¹å·®Varianceï¼š <span class="math display">\[
Var(X)=E[(X-E[X])^2]\\
=E[X^2-2XE[X]+E^2[X]]\\
=E[X^2]-E^2[X]
\]</span> è´å¶æ–¯ã€æœ€å¤§ä¼¼ç„¶ä¼°è®¡åœ¨ä¸‹é¢æœ‰å…¬å¼ï¼Œæ­¤å¤„ä¸èµ˜è¿°ã€‚</p>
<h3 id="ä¼˜åŒ–é—®é¢˜çš„ç§ç±»">ä¼˜åŒ–é—®é¢˜çš„ç§ç±»</h3>
<p>Type of optimization problemsï¼š</p>
<ul>
<li>Continuous vs. Discrete: binary or Integer variables</li>
<li>Linear vs. Nonlinear</li>
<li>Convex vs. nonconvex</li>
</ul>
<p>å¯¹äºå‡¸ä¼˜åŒ–é—®é¢˜(Convex Optimization)</p>
<p>å…¶==Global optimum=Local optimum==.</p>
<p>è¯æ˜å±äºconvex function: <span class="math display">\[
If\ x,y\in \Omega,then\ \theta x+(1-\theta)y\in\Omega\\
f(\theta x+(1-\theta )y)\le\theta f(x)+(1-\theta)f(y)
\]</span></p>
<p><img
src="https://picdm.sunbangyan.cn/2023/11/07/2d8c6cf568bc276e7d3e277d85d4d67e.png" /></p>
<h2 id="ch2-classification">Ch2 Classification</h2>
<p>classificationä¸regressionçš„å¯¹æ¯”ï¼š</p>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 59%" />
</colgroup>
<thead>
<tr class="header">
<th>Classification</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>predict labels</td>
<td>predict values</td>
</tr>
<tr class="even">
<td>categorical, often no order</td>
<td>deal with ordered values</td>
</tr>
<tr class="odd">
<td>all differences are equal</td>
<td>some differences are bigger than others</td>
</tr>
</tbody>
</table>
<p>classifierçš„ä¸€ç§åˆ’åˆ†ï¼š</p>
<ul>
<li>binary äºŒåˆ†ç±»</li>
<li>multi-class å¤šåˆ†ç±»</li>
<li>multi-label å¤šæ ‡ç­¾</li>
</ul>
<p>classifierçš„åˆ’åˆ†ä»¥åŠåº”ç”¨ï¼š</p>
<ul>
<li><p>instance-based(KNN): classifiers use observation directly(no
underlying model)</p>
<blockquote>
<p>K-Nearest neighbours just checks what the class of nearby training
data are</p>
</blockquote></li>
<li><p>generative(Naive Bayes): build a generative statistic model</p>
<blockquote>
<p>NaÃ¯ve Bayes attempts to model the underlying probability
distributions of data</p>
</blockquote></li>
<li><p>discriminative(Decision Tree): directly estimate a decision
rule/boundary</p>
<blockquote>
<p>Decision trees are concerned with establishing the boundaries between
classes</p>
</blockquote></li>
</ul>
<p>æ¬§å¼è·ç¦» Euclidean Distance: <span
class="math inline">\(d(a,b)=\sqrt{\sum(a_i-b_i)^2}\)</span></p>
<p>é©¬æ°è·ç¦»(ä¼°è®¡ä¸è€ƒ)Mahalanobis Distance: <span
class="math inline">\(d(a,b)=\sqrt{(a-b)^TM(a-b)}\)</span></p>
<h3 id="knn">KNN</h3>
<p>Test Errorçš„è®¡ç®—ï¼šå‡è®¾classifieræ˜¯<span
class="math inline">\(h\)</span>ï¼Œé‚£ä¹ˆå¯¹äº<span
class="math inline">\(x_i\)</span>æˆ‘ä»¬çš„é¢„æµ‹ç»“æœæ˜¯<span
class="math inline">\(h(x_i)\)</span>ã€‚å¦‚æœé¢„æµ‹ç»“æœä¸å®é™…çš„ç»“æœ<span
class="math inline">\(y_i\)</span>ä¹‹é—´ä¸åŒï¼Œé‚£ä¹ˆerror<span
class="math inline">\(\epsilon_{te}=\epsilon_{te}+1\)</span>.</p>
<p>å½“<span class="math inline">\(k\)</span>å€¼å˜åŒ–çš„æ—¶å€™ï¼Œè®­ç»ƒè¯¯å·®(<span
class="math inline">\(\epsilon_{tr}\)</span>)å’Œæµ‹è¯•è¯¯å·®(<span
class="math inline">\(\epsilon_{te}\)</span>)åˆä¼šå¦‚ä½•å˜åŒ–ï¼Ÿæœ‰å¦‚ä¸‹ä¸‰ç§æƒ…å†µã€‚</p>
<blockquote>
<p><span class="math inline">\(k\)</span>å€¼ä»£è¡¨çš„æ˜¯å–å‘¨å›´çš„<span
class="math inline">\(k\)</span>ä¸ªé‚»å±…ã€‚</p>
</blockquote>
<ol type="1">
<li>å½“<span class="math inline">\(k\)</span>å€¼è¾ƒå°(ä¾‹å¦‚<span
class="math inline">\(k=1\)</span>)æ—¶ï¼Œæ¨¡å‹çš„å¤æ‚åº¦è¾ƒé«˜ã€‚æ¯ä¸ªæµ‹è¯•æ ·æœ¬åªè€ƒè™‘å…¶æœ€è¿‘çš„ä¸€ä¸ªé‚»å±…ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹å™ªå£°å’Œå±€éƒ¨å˜åŒ–éå¸¸æ•æ„Ÿã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒè¯¯å·®<span
class="math inline">\(\epsilon_{tr}\)</span>å¾ˆä½ï¼Œå› ä¸ºæ¨¡å‹ä¼šå®Œç¾åœ°åŒ¹é…è®­ç»ƒæ ·æœ¬ï¼Œä½†æµ‹è¯•è¯¯å·®<span
class="math inline">\(\epsilon_{te}\)</span>è¾ƒé«˜ï¼Œå› ä¸ºæ¨¡å‹åœ¨æœªè§è¿‡çš„æ ·æœ¬ä¸Šå¯èƒ½æ— æ³•æ³›åŒ–ã€‚</li>
<li>å½“<span class="math inline">\(k\)</span>å€¼è¾ƒå¤§æ—¶(ä¾‹å¦‚<span
class="math inline">\(k=10\)</span>)ï¼Œæ¨¡å‹çš„å¤æ‚åº¦è¾ƒä½ã€‚è€ƒè™‘æ›´å¤šçš„é‚»å±…å¯ä»¥å¹³æ»‘å†³ç­–è¾¹ç•Œå¹¶å‡å°‘å™ªå£°çš„å½±å“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒè¯¯å·®<span
class="math inline">\(\epsilon_{tr}\)</span>å¯èƒ½ä¼šå¢åŠ ï¼Œå› ä¸ºæ¨¡å‹ä¼šæ›´å¤šåœ°è€ƒè™‘å…¶ä»–ç±»åˆ«çš„æ ·æœ¬ï¼Œæµ‹è¯•è¯¯å·®<span
class="math inline">\(\epsilon_{te}\)</span>å¯èƒ½ä¼šå‡å°‘ï¼Œå› ä¸ºæ¨¡å‹åœ¨æœªè§è¿‡çš„æ ·æœ¬ä¸Šæ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å½“<span class="math inline">\(k\)</span>å€¼ç­‰äºæ ·æœ¬æ•°é‡æ—¶(ä¾‹å¦‚<span
class="math inline">\(k=n\)</span>ï¼Œå…¶ä¸­<span
class="math inline">\(n\)</span>æ˜¯æ ·æœ¬æ•°é‡)ï¼Œç›¸å½“äºä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ä½œä¸ºé‚»å±…ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å¤æ‚åº¦è¾ƒä½ï¼Œå› ä¸ºå®ƒä¼šè€ƒè™‘æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„æŠ•ç¥¨ã€‚è®­ç»ƒè¯¯å·®<span
class="math inline">\(\epsilon_{tr}\)</span>å¯èƒ½ä¼šå¢åŠ ï¼Œå› ä¸ºæ¨¡å‹æ›´åŠ ä¿å®ˆã€‚æµ‹è¯•è¯¯å·®<span
class="math inline">\(\epsilon_{te}\)</span>å¯èƒ½ä¼šå‡å°‘ï¼Œå› ä¸ºæ¨¡å‹ä½¿ç”¨äº†æ›´å¤šçš„ä¿¡æ¯ã€‚</li>
</ol>
<p>KNN is a type of lazy learning(å®é™…ä¸Šå¹¶æ²¡æœ‰trainçš„è¿‡ç¨‹)</p>
<blockquote>
<ul>
<li>Lazy: learning only occurs when you see the test example</li>
<li>Eager: learn a model prior to seeing the test example</li>
</ul>
</blockquote>
<p>KNNçš„å¥½å¤„ä¸åå¤„ï¼ˆè¯„ä»·ï¼‰ï¼š</p>
<table>
<colgroup>
<col style="width: 35%" />
<col style="width: 64%" />
</colgroup>
<thead>
<tr class="header">
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Easy to understand</td>
<td>Hard to find a good distance measure</td>
</tr>
<tr class="even">
<td>Very flexible decision boundaries</td>
<td>Irrelevant features and noise reduce performance significantly</td>
</tr>
<tr class="odd">
<td>No learning required</td>
<td>Cannot handle more than a few dozen attributes (Curse of
Dimensionality)</td>
</tr>
<tr class="even">
<td></td>
<td>Computationally expensive</td>
</tr>
</tbody>
</table>
<h4 id="overfittingä¸underfitting">overfittingä¸underfitting</h4>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 44%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>overfitting</th>
<th>underfitting</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ç‰¹å¾</td>
<td>Model could fit any data(trainsetä¸Šç»“æœå¾ˆå¥½), also fit the
noise(testsetä¸Šçš„ç»“æœå¾ˆçƒ‚)</td>
<td>Model is too inflexible to match the data, too few learnable
parameters, wrong learnable
parameters(åœ¨trainsetå’Œtestsetä¸Šè¡¨ç°éƒ½å¾ˆçƒ‚ï¼Œæ ¹æœ¬æ²¡è®­ç»ƒå¥½)</td>
</tr>
<tr class="even">
<td>è§£å†³æ–¹æ³•solution</td>
<td>Less complex model, More data, Regularisation (soft reduction in
parameters)</td>
<td>Better data/pre-processing, More complex model, Less
regularisation</td>
</tr>
</tbody>
</table>
<h4 id="validation-set">Validation set</h4>
<p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆTraining
Setï¼‰ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆTest Setï¼‰ã€‚éªŒè¯é›†ï¼ˆValidation
Setï¼‰æ˜¯ç”¨äºæ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°è°ƒä¼˜çš„ä¸€ä¸ªç‹¬ç«‹æ•°æ®é›†ã€‚</p>
<p>è®­ç»ƒé›†: ç”¨äºæ¨¡å‹çš„å‚æ•°ä¼°è®¡å’Œè®­ç»ƒè¿‡ç¨‹</p>
<p>æµ‹è¯•é›†:ç”¨äºè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½</p>
<p>éªŒè¯é›†:ç”¨äºé€‰æ‹©ä¸åŒæ¨¡å‹å’Œè°ƒæ•´è¶…å‚æ•°çš„è¿‡ç¨‹</p>
<p><strong>Cross validation</strong>: å…¶ä¸­çš„Leave-one-out
cross-validationã€‚å°†æ•°æ®é›†åˆ†ä¸ºå¤šä¸ªå­é›†ï¼Œæ‹¿ä¸€ä¸ªä½œä¸ºvalidationã€‚</p>
<h3 id="naive-bayes">Naive Bayes</h3>
<p><strong>ä¸€äº›èƒŒæ™¯çŸ¥è¯†ï¼š</strong></p>
<p>Bayesçš„limitationsï¼šéœ€è¦æœ‰dataï¼Œæ— dataä¸å·¥ä½œï¼ˆåŸºäºæ•°æ®çš„ç»Ÿè®¡ï¼Œç”Ÿæˆæ¦‚ç‡ï¼‰ã€‚</p>
<p>Bayeså…¬å¼ï¼š<span
class="math inline">\(P(y|x)=\frac{P(y)P(x|y)}{P(x)}\)</span>ï¼Œ<span
class="math inline">\(P(y|x)\)</span>:
è¡¨è¿°çš„æ˜¯åœ¨xäº‹ä»¶å‘ç”Ÿçš„èƒŒæ™¯ä¸‹yäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚</p>
<p>å¦‚æœè¯´æ˜¯ç‹¬ç«‹çš„ï¼š<span
class="math inline">\(P(XY)=P(X)P(Y)\)</span></p>
<p>æå¤§ä¼¼ç„¶ä¼°è®¡ï¼š<span
class="math inline">\(y_{ML}=\mathop{\arg\max}\limits_{y\in
Y}[P(X|y)]\)</span></p>
<p>Naive Bayesï¼šå°†Bayes
Theoremåº”ç”¨äºæœ‰ç€å…ˆéªŒ(Prior)æ¦‚ç‡æˆ–è€…å¯èƒ½æ€§çš„æ•°æ®é›†ä¸­ã€‚</p>
<p>Naiveçš„æ¥ç”±ï¼šæ•°æ®å±æ€§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬å‡è®¾æ˜¯ç‹¬ç«‹çš„ï¼ˆassume
independenceï¼‰ã€‚</p>
<p>Multinomial NaÃ¯ve Bayesï¼š <span class="math display">\[
P(A\&amp;B)=P(A)\times P(B)\\
P(Y|A\&amp;B)=\frac{P(Y)*P(A|Y)*P(B|Y)}{P(X)}
\]</span> å°†æå¤§ä¼¼ç„¶ä¼°è®¡(Maximum likelihood,
å‡è®¾æ•°æ®æ˜¯ä»æŸä¸ªå‚æ•°åŒ–çš„æ¦‚ç‡åˆ†å¸ƒä¸­ç”Ÿæˆçš„ï¼Œæˆ‘ä»¬è¦æ‰¾åˆ°ä½¿å¾—ç»™å®šæ•°æ®çš„æ¦‚ç‡æœ€å¤§åŒ–çš„å‚æ•°å€¼ï¼Œç”¨åå¯¼è®¡ç®—
)åº”ç”¨åœ¨Naive Bayesä¸­ï¼š<span class="math inline">\(\max_\lambda\prod
P(x_i|y;\lambda)\)</span></p>
<p>è€Œè¿™æ ·ä¸å¥½è®¡ç®—ï¼Œäºæ˜¯å˜æˆå¯¹æ•°å‡½æ•°ï¼š<span
class="math inline">\(\max_\lambda\sum_i
log(P(x_i|y;\lambda))\)</span></p>
<blockquote>
<p><span class="math inline">\(P(x_i|y;\lambda)\)</span>: å…¶ä¸­<span
class="math inline">\(\lambda\)</span> ä»£è¡¨å…¶ä¸­çš„å‚æ•°ã€‚</p>
</blockquote>
<h4 id="application">Application</h4>
<p>Bag of words(è¯è¢‹)</p>
<ul>
<li>Xï¼šwords</li>
<li>Yï¼šæ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶(spam)--äºŒåˆ†ç±»é—®é¢˜</li>
</ul>
<p>ä¸¾ä¾‹ï¼šâ€œthe most interesting film this summerâ€</p>
<p><span
class="math inline">\(P(doc|c)=P(the|c)P(most|c)P(interesting|c)P(film|c)P(in|c)P(the|c)P(summer|c)\)</span></p>
<p><strong>æ•°å­¦åŸç†</strong>ï¼š<span class="math inline">\(\hat
c\)</span>æ˜¯æˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„æœ€ç»ˆç»“æœï¼Œä¹Ÿå°±æ˜¯æ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶çš„åˆ¤å®šã€‚ä»¥ä¸‹å…¬å¼çš„æ¨å¯¼æ˜¯åŸºäºæœ´ç´ è´å¶æ–¯å‡è®¾ï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªç±»åˆ«cçš„åéªŒæ¦‚ç‡<span
class="math inline">\(P(c|d)\)</span>æ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ç±»åˆ«ï¼Œå®ç°å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»ã€‚
<span class="math display">\[
P(d|c)&amp;=\prod_{i\in positions}P(w_i|c)\\
\hat c&amp;=\mathop{\arg\max}\limits_{c}P(c|d)\\
&amp;=\mathop{\arg\max}\limits_{c}P(d|c)P(c)\\
&amp;=\mathop{\arg\max}\limits_{c}\prod_{i\in positions}P(w_i|c)P(c)
\]</span></p>
<blockquote>
<ul>
<li>P(d|c)ï¼šè¿™è¡¨ç¤ºåœ¨ç»™å®šç±»åˆ«cçš„æ¡ä»¶ä¸‹ï¼Œæ–‡æ¡£då‡ºç°çš„æ¦‚ç‡ã€‚å®ƒæ˜¯é€šè¿‡å‡è®¾æ–‡æ¡£ä¸­çš„æ¯ä¸ªè¯ï¼ˆ<span
class="math inline">\(w_i\)</span>ï¼‰åœ¨ç»™å®šç±»åˆ«cçš„æ¡ä»¶ä¸‹æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”é€šè¿‡å¯¹è¿™äº›æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯æ¥è®¡ç®—çš„ã€‚å…·ä½“è€Œè¨€ï¼Œè¿™ä¸ªå…¬å¼ä½¿ç”¨äº†ä¸€ä¸ªä½ç½®é›†åˆï¼ˆpositionsï¼‰ï¼Œè¡¨ç¤ºæ–‡æ¡£ä¸­è¢«è€ƒè™‘çš„è¯çš„ä½ç½®ï¼Œç„¶åè®¡ç®—æ¯ä¸ªä½ç½®ä¸Šè¯<span
class="math inline">\(w_i\)</span>åœ¨ç»™å®šç±»åˆ«cçš„æ¡ä»¶ä¸‹çš„æ¦‚ç‡<span
class="math inline">\(P(w_i|c)\)</span>ï¼Œå¹¶å°†å®ƒä»¬ç›¸ä¹˜å¾—åˆ°<span
class="math inline">\(P(d|c)\)</span>ã€‚</li>
<li><span class="math inline">\(\hat
c\)</span>ï¼šè¿™è¡¨ç¤ºé€šè¿‡è´å¶æ–¯å†³ç­–è§„åˆ™ï¼ˆBayes' decision
ruleï¼‰é€‰æ‹©çš„æœ€æœ‰å¯èƒ½çš„ç±»åˆ«ã€‚åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—æ¯ä¸ªç±»åˆ«cçš„åéªŒæ¦‚ç‡<span
class="math inline">\(P(c|d)\)</span>æ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ç±»åˆ«ã€‚æ ¹æ®è´å¶æ–¯å®šç†ï¼Œ<span
class="math inline">\(P(c|d)\)</span>å¯ä»¥é€šè¿‡<span
class="math inline">\(P(d|c)P(c)\)</span>è®¡ç®—å¾—åˆ°ã€‚</li>
<li><span
class="math inline">\(P(c)\)</span>ï¼šè¿™è¡¨ç¤ºç±»åˆ«cåœ¨æ•´ä¸ªæ•°æ®é›†ä¸­å‡ºç°çš„æ¦‚ç‡ï¼Œä¹Ÿç§°ä¸ºç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ã€‚å®ƒå¯ä»¥é€šè¿‡è®¡ç®—åœ¨è®­ç»ƒæ•°æ®ä¸­ç±»åˆ«cçš„é¢‘ç‡æ¥ä¼°è®¡ã€‚</li>
</ul>
</blockquote>
<p>æ¥ä¸‹æ¥è¦å¦‚ä½•è®¡ç®—å‘¢ï¼Ÿè¿˜æ˜¯ä¸€æ ·çš„ï¼Œ<span
class="math inline">\(\prod\)</span>ä¸å¥½å¤„ç†ï¼Œæˆ‘ä»¬è½¬æ¢ä¸ºlog+<span
class="math inline">\(\sum\)</span>ã€‚ <span class="math display">\[
\prod_{i\in positions}P(w_i|c)P(c)&amp;=\sum_{i\in
positions}log(P(w_i|c))+log(P(c))\\
&amp;=\sum_{k\in|V|}n_klog(P(w_i|c))+log(P(c))
\]</span> <span class="math inline">\(n_k\)</span> : The number of
occurrences of the k th word in the vocabulary. It can be zero or
non-zero. è¯æ±‡è¡¨ä¸­<span class="math inline">\(w_i\)</span>çš„è®¡æ•°ï¼Œ<span
class="math inline">\(w_i\)</span>ä»£è¡¨æ–‡æ¡£documentä¸­å…¶ä¸­çš„æŸä¸ªwordã€‚</p>
<p><span class="math inline">\(|V|\)</span>: The list of vocabulary,
è¯æ±‡è¡¨çš„å¤§å°</p>
<h3 id="decision-tree">Decision Tree</h3>
<p>ä¸€ä¸ªå†³ç­–çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®šï¼š</p>
<ul>
<li>Which variable to test</li>
<li>What to test it against</li>
<li>What function to apply to a variable prior to a test</li>
</ul>
<p>å†³ç­–æ ‘æ¨¡æ‹Ÿäº†ä¸€ä¸ªæ ‘å½¢ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹è¡¨ç¤ºå¯¹ä¸€ä¸ªç‰¹å¾çš„æµ‹è¯•ï¼Œæ¯ä¸ªåˆ†æ”¯ä»£è¡¨æµ‹è¯•çš„ç»“æœï¼Œæ¯ä¸ªå¶èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç±»åˆ«æˆ–å›å½’å€¼ã€‚å®ƒçš„æ„å»ºè¿‡ç¨‹ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€‰æ‹©æœ€ä½³çš„ç‰¹å¾è¿›è¡Œæµ‹è¯•ã€‚é€‰æ‹©çš„ä¾æ®å¯ä»¥æ˜¯ä¿¡æ¯å¢ç›Šï¼ˆInformation
Gainï¼š<span
class="math inline">\(Entropy=\sum-p_ilog(p_i)\)</span>ï¼‰ã€åŸºå°¼ç³»æ•°ï¼ˆGini
Impurityï¼‰æˆ–å…¶ä»–åº¦é‡ã€‚é€šè¿‡æµ‹è¯•ç‰¹å¾å°†æ•°æ®é›†åˆ’åˆ†ä¸ºä¸åŒçš„å­é›†ï¼Œç„¶åé€’å½’åœ°åœ¨æ¯ä¸ªå­é›†ä¸Šé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ¡ä»¶ï¼Œå¦‚è¾¾åˆ°æœ€å¤§æ·±åº¦ã€èŠ‚ç‚¹åŒ…å«çš„æ ·æœ¬æ•°å°äºæŸä¸ªé˜ˆå€¼æˆ–èŠ‚ç‚¹ä¸­çš„æ ·æœ¬å±äºåŒä¸€ä¸ªç±»åˆ«ã€‚</p>
<ul>
<li>åˆ†ç±»é—®é¢˜ä¸­ï¼šå†³ç­–æ ‘çš„å¶èŠ‚ç‚¹è¡¨ç¤ºä¸€ä¸ªç±»åˆ«ï¼Œé€šè¿‡åœ¨æ ‘ä¸Šä»æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹çš„è·¯å¾„ï¼Œå†³ç­–æ ‘å¯ä»¥å¯¹æ–°çš„æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å›å½’é—®é¢˜ä¸­ï¼šå¶èŠ‚ç‚¹è¡¨ç¤ºä¸€ä¸ªé¢„æµ‹å€¼ï¼Œé€šè¿‡åœ¨æ ‘ä¸Šä»æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹çš„è·¯å¾„ï¼Œå†³ç­–æ ‘å¯ä»¥å¯¹æ–°çš„æ ·æœ¬è¿›è¡Œå›å½’é¢„æµ‹ã€‚</li>
</ul>
<h2 id="ch3-regression">Ch3 Regression</h2>
<p>å¯¹regressionä¸classificationçš„åŒºåˆ†ï¼š</p>
<p>classificationä¸­Yæ˜¯ç¦»æ•£å˜é‡ï¼Œregressionä¸­Yæ˜¯è¿ç»­å˜é‡ã€‚</p>
<p><strong>Application</strong></p>
<ul>
<li>æˆ¿ä»·é¢„æµ‹</li>
<li>å›¾åƒå¤„ç†(Image processing)</li>
<li>äººæ•°ç»Ÿè®¡(Crowd counting)</li>
<li>ç”Ÿæˆå£°éŸ³(Generate sounding)</li>
</ul>
<h3 id="linear-regression">Linear Regression</h3>
<p>çº¿æ€§å›å½’ï¼šç»™å®šä¸€äº›æ ·æœ¬ç‚¹ï¼Œæ‰¾å‡ºä¸€æ¡å¯ä»¥æ‹Ÿåˆçš„æ›²çº¿ã€‚ <span
class="math display">\[
y=&amp;f(x)\\
y=&amp;w^Tx=\sum^D_{k=1}w_kx_k\\
err=&amp;(f(x)-\hat y)^2\\
\mathcal{L}=&amp;\sum^N_{i=1}(f(x_i)-\hat y_i)^2\\
=&amp;\sum^N_{i=1}(w^Tx_i-\hat y_i)^2\\
=&amp;||w^TX-\hat y||^2
\]</span> è®¡ç®—<span class="math inline">\(\min
\mathcal{L}\)</span>æ±‚å…¶å¯¹wçš„å¯¼æ•°=0,<span
class="math inline">\(w^*=\mathop{\arg\min}\limits_{w}\mathcal{L}\)</span>ï¼š
<span class="math display">\[
\frac{\partial\mathcal{L}}{\partial w}&amp;=0\\
\frac{\partial{||w^T X-\hat y||^2}}{\partial w}&amp;=0\\
w&amp;=(XX^T)^{-1}X\hat y^T
\]</span> ä¸Šè¿°çš„<span class="math inline">\(\hat
y\)</span>ä»£è¡¨æœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå€¼scalarï¼Œä½†å¦‚æœæœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå‘é‡ï¼Œåˆ™å†™æˆ<span
class="math inline">\(\hat Y\)</span>ã€‚ <span class="math display">\[
w=(XX^T)^{-1}X\hat Y^T
\]</span></p>
<p>ä¸èƒ½ç”¨Regressionå»Classificationçš„ç†ç”±ï¼š</p>
<ul>
<li><p>Limitation:</p>
<p>â€“ Put unnecessary requirements to the predicted output</p>
<p>â€“ May increase the fitting difficulty and lead to bad training
result</p></li>
<li><p>But why is it commonly used in practice?</p>
<p>â€“ Close-form solution, less storage for low</p>
<p>-dimensional data</p>
<p>â€“ Quick update for incremental learning, distributed
learning</p></li>
</ul>
<h3 id="regularized-regression">Regularized Regression</h3>
<p>å³æ·»åŠ äº†æ­£åˆ™åŒ–æƒ©ç½šé¡¹regularizationçš„Linear Regressionã€‚</p>
<p>regularizationçš„ç”¨é€”ï¼Ÿ</p>
<ul>
<li>é¿å…è¿‡æ‹Ÿåˆ avoid overfitting</li>
<li>Enforce certain property of solution</li>
</ul>
<p>å…¶å½¢å¼ï¼š<span class="math inline">\(\mathcal{L}=||w^TX-\hat
y||^2+\Omega(w)\)</span>, <span
class="math inline">\(\Omega(w)\)</span>ä¸ºæ­£åˆ™åŒ–æƒ©ç½šé¡¹ã€‚</p>
<blockquote>
<p>å¯¹p-èŒƒå¼çš„ä»‹ç»(p-Norm)ï¼š <span class="math display">\[
||x||_p=(\sum^d_{j=1}|x^j|^p)^{\frac 1 p}
\]</span> å½“p=1çš„æ—¶å€™ï¼šLasso Regression</p>
<p>å½“p=2çš„æ—¶å€™ï¼šRidge Regression</p>
</blockquote>
<h4 id="ridge-regression">Ridge Regression</h4>
<p>å…¶Loss functionçš„å½¢å¼æ˜¯ï¼š <span class="math display">\[
\mathcal{L}=||w^TX-\hat y||^2_2+\lambda||w||^2_2
\]</span> å¯¹å…¶æ±‚è§£ï¼š <span class="math display">\[
w=(XX^T+\lambda I)^{-1}X\hat y^T
\]</span> å¦‚æœ<span
class="math inline">\(XX^T\)</span>ä¸æ˜¯å¯é€†çš„ï¼Œå¯èƒ½å°±ä¸èƒ½å†™æˆ<span
class="math inline">\((XX^T)^{-1}\)</span>ï¼Œè¿™æ„å‘³ç€æ­¤å¤„æœ‰å¤šç§æ–¹æ³•å»å¤„ç†ï¼ˆè€Œä¸æ˜¯æ±‚é€†ï¼‰ã€‚å¦‚æœåŠ ä¸Šä¸€ä¸ªæ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆå®ƒæ€»æ˜¯å¯é€†çš„ã€‚It
essentially provides a criterion for choosing the optimal solution among
multiple equivalent solutions of the first-termã€‚</p>
<h4 id="lasso-regression">Lasso Regression</h4>
<p>å…¶Loss Functionçš„å½¢å¼æ˜¯ï¼š <span class="math display">\[
\mathcal{L}=||w^TX-\hat y||^2_2+\lambda||w||_1
\]</span></p>
<ul>
<li>L1 norm encourages sparse solution. This could be useful for
understanding the impact of various factors, e.g. perform feature
selection.</li>
<li>Sometimes it can lead to an improved performance since it can
<strong>suppressing noisy factors</strong>. å…¶åŸå› æ˜¯ï¼šè®¡ç®—å‡ºæ¥<span
class="math inline">\(2XX^Tw=2X\hat
y^T-\lambda\)</span>ã€‚ä½¿å¾—æŸäº›ç‰¹å¾çš„<span
class="math inline">\(w\)</span>å¯ä»¥å–0ã€‚</li>
<li>Unfortunately, it does not have a close-form solution.</li>
</ul>
<h3 id="support-vector-regression">Support Vector Regression</h3>
<p>Key idea: if the fitting error is already small enough, do not make
it smaller.</p>
<p>SVRé€šè¿‡åœ¨äºŒç»´ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜è¶…å¹³é¢æ¥å®ç°å¯¹å›å½’è¿‡ç¨‹çš„å»ºæ¨¡ã€‚ç”±äºè¿™ä¸ªæœ€ä¼˜è¶…å¹³é¢ä»…è€ƒè™‘åˆ°äº†åœ¨è®­ç»ƒé›†å‘¨å›´è¾¹ç¼˜çš„ç‚¹ï¼Œä½¿å¾—æ¨¡å‹å¯¹æ•°æ®ç‚¹çš„è¿‡æ‹Ÿåˆç°è±¡è¿›è¡Œæœ‰æ•ˆåœ°é¿å…ã€‚åŒæ—¶ï¼Œæ ¹æ®å†æŠ•å½±è¯¯å·®ä½œä¸ºæƒ©ç½šé¡¹çš„å¤æ‚åº¦æ§åˆ¶å‚æ•°å¯ä»¥å¾ˆå¥½åœ°è°ƒèŠ‚å›å½’æ¨¡å‹çš„çµæ´»æ€§ã€‚</p>
<p>ç¡¬é—´éš”ï¼ˆHard-marginï¼‰: <span class="math display">\[
&amp;\min \frac 1 2 ||w||^2\\
s.t. &amp;y_i-wx_i-b\le\epsilon\\
&amp;w_ix_i+b-y_i\le\epsilon
\]</span> è½¯é—´éš”ï¼ˆSoft-marginï¼‰ï¼š <span class="math display">\[
\min \frac 1 2 ||w||^2+C\sum^m_{i=1}(\xi_i+\xi_i^*)\\
s.t.\begin{cases}
&amp;y_i-wx_i-b\le\epsilon+\xi_i\\
&amp;wx_i+b-y_i\le\epsilon+\xi_i^*\\
&amp;\xi_i,\xi_i^*\ge0,i=1,\dots,m
\end{cases}
\]</span> SVR: <span class="math display">\[
\min_{w,b}\frac 1 2 ||w||^2+\sum_{i}\max(1-y_i(w^Tx_i+b),0)
\]</span> <span
class="math inline">\(\max(1-y_i(w^Tx_i+b),0)\)</span>è¡¨ç¤ºåœ¨<span
class="math inline">\(1-y_i(w^Tx_i+b)\)</span>ä¸0ä¹‹é—´å–å¤§çš„é‚£ä¸ªã€‚</p>
<p>å¦‚æœ<span
class="math inline">\((w^Tx_i+b)\)</span>æ˜¯binaryçš„ï¼šé‚£ä¹ˆå‡è®¾å¯¹äºpositive
classæ˜¯æ­£æ•°ï¼Œå¯¹äºnegative class æ˜¯è´Ÿæ•°ã€‚å½“decision
valueä¸å¤Ÿå¤§ï¼ˆæˆ–è€…æ˜¯ä¸å¤Ÿå°ï¼‰çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±ä¼šâ€œæ¿€æ´»â€è¿™ä¸ªæƒ©ç½šé¡¹ã€‚</p>
<h4 id="dual-form-of-svr">Dual form of SVR</h4>
<p>åŸå§‹é—®é¢˜ï¼š <span class="math display">\[
\min \frac 1 2 ||w||^2+C\sum^m_{i=1}(\xi_i+\xi_i^*)\\
s.t.\begin{cases}
&amp;y_i-wx_i-b\le\epsilon+\xi_i\\
&amp;wx_i+b-y_i\le\epsilon+\xi_i^*\\
&amp;\xi_i,\xi_i^*\ge0,i=1,\dots,m
\end{cases}
\]</span> å¯¹å¶é—®é¢˜ï¼ˆdual problemï¼‰ï¼š <span class="math display">\[
\max=
\begin{cases}
\frac 1 2
\sum^{m}_{i,j=1}(\alpha_i-\alpha_i^*)(\alpha_j-\alpha_j^*)\langle
x_i,x_j \rangle\\
-\epsilon
\sum^m_{i=1}(\alpha_i+\alpha_i^*)+\sum^m_{i=1}y_i(\alpha_i-\alpha_i^*)
\end{cases}\\
s.t. \sum^m_{i=1}(\alpha_i-\alpha_i^*)=0;0\le \alpha_i,\alpha_i^*\le C
\]</span></p>
<h2 id="ch45-linear-classification-and-svm">Ch4&amp;5 Linear
Classification and SVM</h2>
<p>å±äºstatistical MLã€‚</p>
<p><strong>Primal Problem VS Dual Problem</strong>: åŸå§‹é—®é¢˜is
hardã€‚å¯¹å¶é—®é¢˜çš„è§£æ°å¥½å¯¹åº”ç€åŸå§‹é—®é¢˜çš„è§£ï¼Œå› æ­¤åªè¦èƒ½è§£å†³å¯¹å¶é—®é¢˜ï¼Œå°±æ„å‘³ç€æˆ‘ä»¬è§£å‡ºäº†åŸå§‹é—®é¢˜ã€‚</p>
<h3 id="linear-classifiers">Linear Classifiers</h3>
<p>åœ¨çº¿æ€§çš„åˆ†ç±»å™¨ä¸­å¤„ç†åˆ†ç±»ï¼Œå°±æ˜¯è®¡ç®—ç‰¹å¾çš„çº¿æ€§ç»„åˆï¼š <span
class="math display">\[
s_c=\sum_iw_i^cx_i+b_c\\
s_c=w_c^Tx+b_c
\]</span> å¦‚æœæ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼šç»“æœå¤§äº0ï¼Œå±äºclass 1ï¼Œç»“æœå°äº0ï¼Œå±äºclass
2ã€‚</p>
<h3 id="convex-theorem">Convex Theorem</h3>
<p>==ç”¨äºè¯æ˜ä¸€ä¸ªfunctionæ˜¯å¦æ˜¯convexçš„==</p>
<p>å¦‚æœä¸€ä¸ªå‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œé‚£ä¹ˆå®ƒå°†æ»¡è¶³ï¼š $$</p>
<p>$$</p>
<h3 id="svm">SVM</h3>
<p>ç›®æ ‡ï¼šfind a hyperplane <span class="math inline">\(w^Tx-b=0\)</span>
ä½¿å¾—è·ç¦»ä¸¤ç±»samplesçš„è·ç¦»éƒ½æ¯”è¾ƒè¿œã€‚</p>
<h4 id="hard-margin">Hard-margin</h4>
<p>å› æ­¤æˆ‘ä»¬ä¸€å¼€å§‹çš„goal functionæ˜¯ï¼š<span class="math inline">\(\max
\frac 2 {||w||}\)</span></p>
<p>è¯¥å½¢å¼å¯ä»¥è½¬åŒ–æˆ: <span class="math display">\[
\min \frac1 2 ||w||^2\\
s.t.\ y_i*(w\cdot x+b)\ge 1
\]</span>
è¯¥å½¢å¼ç¬¦åˆå‡¸ä¼˜åŒ–ç†è®ºï¼Œå¯ä»¥ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•è§£å†³é—®é¢˜ï¼ˆLagrangian Dual
Problemï¼‰ <span class="math display">\[
\mathcal{L}=\frac 1 2||w||^2-\sum\alpha_i[y_i*(w\cdot x_i+b)-1]
\]</span> <span class="math inline">\(\alpha_i\)</span>: Lagrange
Multiplier</p>
<p>å¯¹<span class="math inline">\(\mathcal{L}\)</span>æ±‚è§£åå¯¼ï¼š <span
class="math display">\[
\frac{\partial \mathcal{L}}{\partial w}=w-\sum\alpha_iy_ix_i=0\\
w=\sum\alpha_iy_ix_i\\\\
\frac{\partial \mathcal{L}}{\partial b}=\sum\alpha_iy_i=0
\]</span></p>
<ul>
<li><span class="math inline">\(w\)</span> : is a weighted sum of the
input vectors (<span class="math inline">\(x_i\)</span> )</li>
<li>å¾ˆå¤šæƒ…å†µä¸‹<span class="math inline">\(\alpha_i=0\)</span>: because
this point doesnâ€™t contribute to the margin</li>
</ul>
<p>å†æŠŠ<span
class="math inline">\(w=\sum\alpha_iy_ix_i\)</span>ä»£è¿›<span
class="math inline">\(\mathcal{L}\)</span>ä¸­ï¼Œä½¿å¾—<span
class="math inline">\(\mathcal{L}\)</span>ä¸­æ²¡æœ‰<span
class="math inline">\(w\)</span>ï¼Œåªæœ‰<span
class="math inline">\(\alpha_i\)</span>ä¸€ä¸ªå‚æ•°ã€‚ <span
class="math display">\[
\mathcal{L}=&amp;\frac 1 2w^Tw-\sum\alpha_i[y_i*(w\cdot x_i+b)-1]\\
=&amp;\frac 1 2
(\sum\alpha_iy_ix_i)(\sum\alpha_jy_jx_j)-\sum\alpha_i[y_i\sum\alpha_jy_jx_j\cdot
x_i]+b\sum\alpha_iy_i+\sum\alpha_i\\
\]</span> å› ä¸ºï¼š<span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial b}=\sum\alpha_iy_i=0\)</span> <span
class="math display">\[
\mathcal{L}=&amp;\frac 1 2
(\sum\alpha_iy_ix_i)(\sum\alpha_jy_jx_j)-\sum\alpha_i[y_i\sum\alpha_jy_jx_j\cdot
x_i]+\sum\alpha_i\\
=&amp;\sum\alpha_i-\frac 1 2 (\sum\alpha_iy_ix_i)(\sum\alpha_jy_jx_j)\\
=&amp;\sum\alpha_i-\frac 1 2 \sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j
\]</span></p>
<h4 id="soft-margin">Soft-margin</h4>
<p>å…¶goal functionï¼š <span class="math display">\[
\min \frac 1 2 ||w||^2+C\sum^m_i\xi_i\\
s.t.\ y_i*(w\cdot x+b)\ge 1-\xi_i\\
\xi_i\ge0
\]</span> å¯¹äºå‚æ•°<span class="math inline">\(C\)</span>:</p>
<ul>
<li>Low C: we donâ€™t pay anything for these violations (width is
king)</li>
<li>High C: we pay a lot for violations (no violations is king)</li>
</ul>
<blockquote>
<p>çªç„¶åœ¨PPTä¸­ä»‹ç»äº†ä¸€ä¸‹hinge lossâ€”â€”</p>
<p>Hinge Loss: ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ã€‚Hinge
Lossç”¨äºè¡¡é‡æ ·æœ¬çš„åˆ†ç±»é”™è¯¯å’Œåˆ†ç±»è¾¹ç•Œçš„é—´éš”ã€‚å…¶åœ¨soft-marginä¸­çš„å®šä¹‰å¦‚ä¸‹ï¼š
<span class="math display">\[
\min \frac 1 2||w||^2+C\sum^m_i\max(0,1-y_i*(w\cdot x+b))
\]</span> <span class="math inline">\(y_i\)</span>:
è¡¨ç¤ºæ ·æœ¬çš„çœŸå®æ ‡ç­¾ï¼ˆé€šå¸¸ä¸º-1æˆ–1ï¼‰</p>
<p><span class="math inline">\(w\cdot x+b\)</span>:
è¡¨ç¤ºæ ·æœ¬çš„é¢„æµ‹åˆ†ç±»ï¼ˆå³å†³ç­–å‡½æ•°è¾“å‡ºçš„å€¼ï¼‰ã€‚</p>
<p>Hinge
Lossçš„ç›®æ ‡æ˜¯ä½¿æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬çš„æŸå¤±ä¸º0ï¼Œå¹¶å¢å¤§é”™è¯¯åˆ†ç±»æ ·æœ¬çš„æŸå¤±ã€‚åœ¨è½¯é—´éš”åˆ†ç±»ä¸­ï¼ŒHinge
Lossé€šå¸¸ä¸æ­£åˆ™åŒ–é¡¹ç»“åˆä½¿ç”¨ï¼Œä»¥å¹³è¡¡åˆ†ç±»é”™è¯¯å’Œæ¨¡å‹å¤æ‚åº¦ã€‚é€šè¿‡æœ€å°åŒ–Hinge
Losså’Œæ­£åˆ™åŒ–é¡¹ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªå…·æœ‰è¾ƒå°é—´éš”è¿è§„å’Œè¾ƒå°æ¨¡å‹å¤æ‚åº¦çš„åˆ†ç±»æ¨¡å‹ï¼Œä»è€Œåœ¨è®­ç»ƒé›†ä¸Šå’Œæµ‹è¯•é›†ä¸Šè·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p>æ¥ä¸‹æ¥ç»§ç»­æˆ‘ä»¬soft-marginçš„æ±‚è§£éƒ¨åˆ†ï¼š <span class="math display">\[
\mathcal{L}=\frac 1 2
||w||^2+C\sum^m_i\xi_i+\sum_i\alpha_i(1-\xi_i-y_i(w^Tx_i+b))-\sum_i\beta_i\xi_i
\]</span> æ±‚è§£åå¯¼ï¼Œå› ä¸ºåªæ˜¯åŠ äº†å…¶ä»–é¡¹ï¼Œæ‰€ä»¥<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
w}\)</span>ä¸å˜ï¼Œæ­¤å¤„æ±‚è§£<span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial \xi_i}\)</span> <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \xi_i}=C-\alpha_i-\beta_i=0\\
\alpha_i=C-\beta_i
\]</span> å…¶ä»–çš„<span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial w}\)</span>ä»¥åŠ<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
b}\)</span>å‡ä¸å˜ã€‚</p>
<p>KKTæ¡ä»¶ï¼š <span class="math display">\[
\begin{cases}
stationarity:&amp;\nabla\mathcal{L}(x^*,{\mu_i})=\nabla
f(x^*)+\sum_i\mu_i\nabla g_i(x^*)=0\\
primary\ feasibility:&amp;g_i(x^*)\le 0\\
dual\ feasibility:&amp;\mu_i^*\ge0\\
complementary\ slackness:&amp;\mu_i^*g_i(x^*)=0
\end{cases}
\]</span> å¯¹æ¯”ä¸€ä¸‹hard-marginä»¥åŠsoft-marginçš„æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š <span
class="math display">\[
\mathcal{L}=\frac 1 2||w||^2-\sum\alpha_i[y_i*(w\cdot x_i+b)-1]\\
\mathcal{L}=\frac 1 2
||w||^2+C\sum^m_i\xi_i+\sum_i\alpha_i(1-\xi_i-y_i(w^Tx_i+b))-\sum_i\beta_i\xi_i
\]</span> æˆ‘ä»¬å®¹æ˜“å‘ç°ï¼š<span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial w}\)</span>ä»¥åŠ<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
b}\)</span>å‡ä¸å˜ã€‚</p>
<p>å¯¹äºsoft-marginå°†<span
class="math inline">\(w=\sum\alpha_iy_ix_i\)</span>ä»£è¿›å»ï¼š <span
class="math display">\[
\mathcal{L}=&amp;\frac 1 2
||w||^2+C\sum^m_i\xi_i+\sum_i\alpha_i(1-\xi_i-y_i(w^Tx_i+b))-\sum_i\beta_i\xi_i\\
=&amp;\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j+C\sum^m_i\xi_i+\sum_i\alpha_i-\sum_i\alpha_i\xi_i-\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j+b\sum_i\alpha_iy_i-\sum_i\beta_i\xi_i\\
\]</span> å› ä¸ºï¼š<span
class="math inline">\(\sum_i\alpha_i\xi_i+\sum_i\beta_i\xi_i=C\sum_i^m\xi_i\)</span>(<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
\xi_i}=C-\alpha_i-\beta_i=0\)</span>)ä»¥åŠ<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
b}=\sum\alpha_iy_i=0\)</span> <span class="math display">\[
\mathcal{L}=\sum\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j
\]</span> ä½ ä¼šå‘ç°soft-marginè®¡ç®—å‡ºæ¥çš„<span
class="math inline">\(\mathcal{L}\)</span>å’Œhard-marginçš„ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚</p>
<p>å› æ­¤å¯¹å¶é—®é¢˜æ˜¯ï¼š <span class="math display">\[
\max_{\alpha_i}[\sum_i\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j]\\
s.t.\ 0\le\alpha_i\le C,\sum_i\alpha_iy_i=0
\]</span> æˆ‘ä»¬çš„åŸé—®é¢˜æ˜¯ï¼š <span class="math display">\[
\min \frac 1 2 ||w||^2+C\sum^m_i\xi_i\\
s.t.\ y_i*(w\cdot x+b)\ge 1-\xi_i\\
\xi_i\ge0
\]</span> æ€ä¹ˆä»å¯¹å¶é—®é¢˜è½¬åŒ–ä¸ºåŸé—®é¢˜å‘¢ï¼Ÿ</p>
<p><span class="math inline">\(w^*\)</span>æ¯”è¾ƒç®€å•ï¼š <span
class="math display">\[
w^*=\sum\alpha_i^*y_ix_i
\]</span> å¯¹äº<span class="math inline">\(b\)</span>ï¼š</p>
<ul>
<li>Find a $_i^*(0, ğ¶) $vector</li>
<li>Thus, <span class="math inline">\(y_i(w^Tx_i+b^*)=1\)</span></li>
<li>Thus, <span class="math inline">\(b^*=y_i-w^Tx_i\)</span></li>
</ul>
<p>åœ¨soft
marginçš„æƒ…å†µä¸‹ï¼Œç”±äºå­˜åœ¨ä¸€äº›æ ·æœ¬è½åœ¨é—´éš”è¾¹ç•Œå†…éƒ¨ï¼Œå› æ­¤é€‰æ‹©å¤šä¸ªæ”¯æŒå‘é‡è®¡ç®—åç½®bå¯èƒ½æ›´åˆé€‚ã€‚ä¸€ç§å¸¸è§çš„åšæ³•æ˜¯é€‰æ‹©æ‰€æœ‰æ»¡è¶³<span
class="math inline">\(0 &lt; \alpha_i &lt;
C\)</span>çš„æ”¯æŒå‘é‡ï¼Œå¹¶è®¡ç®—å®ƒä»¬çš„å¹³å‡å€¼ä½œä¸ºåç½®bã€‚</p>
<p><span class="math display">\[
\mu_i^*g_i(x^*)=0\ \forall i\\
\]</span></p>
<h2 id="ch6-pca-lda-and-dimensionality-reduction">Ch6 PCA, LDA and
Dimensionality reduction</h2>
<h3 id="dimensionality-reduction">Dimensionality reduction</h3>
<p>å…¶åŸºæœ¬åŸç†æ˜¯ï¼šPreserve â€œusefulâ€ information in low dimensional
data.</p>
<p>å…¶å¸¸ç”¨çš„æ–¹æ³•ï¼šPCAï¼ŒLDA</p>
<p>reasons:</p>
<ul>
<li>Extract underlying factors</li>
<li>Reduce data noise
<ul>
<li>Face recognition</li>
<li>Applied to image de-noising</li>
</ul></li>
<li>Reduce the number of model parameters
<ul>
<li>Avoid over-fitting</li>
<li>Reduce computational cost</li>
</ul></li>
<li>Visualization</li>
</ul>
<h3 id="pcaprincipal-component-analysis">PCA(Principal Component
Analysis)</h3>
<p>==æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚==</p>
<p>PCAï¼š</p>
<ul>
<li>Transform data to remove redundant information</li>
<li>Keep the most informative dimensions after the transformation</li>
</ul>
<p>å…¶æ­¥éª¤:</p>
<ul>
<li><p>å»ä¸­å¿ƒåŒ–(De-correlating data): Correlation can be removed by
rotating the data point or coordinate</p></li>
<li><p>è®¡ç®—åæ–¹å·®çŸ©é˜µï¼Œæ‰¾ç‰¹å¾å‘é‡ä¸ç‰¹å¾å€¼(Eigen decomposition) <span
class="math display">\[
A=Q\Lambda Q^T\\
QQ^T=Q^TQ=I
\]</span> æ­¤å¤„çš„<span
class="math inline">\(I\)</span>æ˜¯å•ä½çŸ©é˜µï¼ŒAæ˜¯å¯¹ç§°çŸ©é˜µï¼Œå¼2å¼æ­£äº¤çŸ©é˜µQçš„æ€§è´¨ï¼Œ<span
class="math inline">\(\Lambda\)</span>æ˜¯åæ–¹å·®çŸ©é˜µã€‚</p></li>
</ul>
<p>è‹±æ–‡ç‰ˆï¼š</p>
<ol type="1">
<li>Subtract mean</li>
<li>Calculate the covariance matrix</li>
<li>Calculate eigenvectors and eigenvalues of the covariance matrix</li>
<li>Rank eigenvectors by its corresponding eigenvalues</li>
<li>Obtain P with its row vectors corresponding to the top k
eigenvectors</li>
</ol>
<p>å…¶æ•°å­¦åŸç†ï¼š<span
class="math inline">\(X_c\)</span>æ˜¯ä¸­å¿ƒåŒ–æ•°æ®çŸ©é˜µï¼ˆæ¯ä¸ªæ•°æ®å‡å»å…¶å‡å€¼å‘é‡ä¹‹åçš„ç»“æœï¼‰</p>
<p>å› æ­¤åŸå§‹çš„åæ–¹å·®çŸ©é˜µä¸ºï¼š <span class="math display">\[
C_X=\frac 1 n X_cX_c^T
\]</span> æˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ä¸ªçŸ©é˜µP,å¯ä»¥å¯¹æ•°æ®å»ä¸­å¿ƒåŒ–ï¼š <span
class="math display">\[
Y=PX_c\\
C_Y=\frac 1 n PX_c(PX_c)^T=\frac 1 nPX_cX_c^TP^T
\]</span> æœ‰å› ä¸ºï¼š<span class="math inline">\(C_X=\frac 1 n
X_cX_c^T\)</span>ï¼Œä»£è¿›å»å¾—ï¼š <span class="math display">\[
C_Y=\frac 1 nPX_cX_c^TP^T=\frac 1 {n^2}PC_XP^T
\]</span> å¯¹<span
class="math inline">\(X_cX_c^T\)</span>è¿›è¡Œç‰¹å¾åˆ†è§£ï¼š<span
class="math inline">\(X_cX_c^T=Q\Lambda Q^T\)</span> <span
class="math display">\[
C_Y=\frac 1 nPX_cX_c^TP^T=\frac 1 nPQ\Lambda Q^TP^T
\]</span> ä»¤<span class="math inline">\(P=Q^T\)</span>,<span
class="math inline">\(C_Y=\frac 1 n \Lambda\)</span></p>
<p>High dimensionality issueï¼ˆKernel PCAï¼‰</p>
<ol type="1">
<li>Centralize data</li>
<li>Calculate the kernel matrix</li>
<li>Perform Eigen-decomposition on the kernel matrix and obtain its
eigenvector <span class="math inline">\(V\)</span></li>
<li>Obtain the Eigenvector of the covariance matrix by <span
class="math inline">\(u=Xv\)</span></li>
</ol>
<h3 id="ldalinear-discriminative-analysis">LDA(Linear Discriminative
Analysis)</h3>
<p><strong>æœ‰ç›‘ç£å­¦ä¹ </strong>ï¼šåˆ©ç”¨äº†æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ä¿¡æ¯æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œåˆ†ç±»ã€‚</p>
<p>Supervised information ï¼š</p>
<ol type="1">
<li>Class label</li>
<li>Data from the same class =&gt; Become close</li>
<li>Data from different classes =&gt; far from each other</li>
</ol>
<p>LDAå‡è®¾æ•°æ®æ»¡è¶³é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”æ ¹æ®ç±»åˆ«ä¿¡æ¯è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚å®ƒçš„ç›®æ ‡æ˜¯é€šè¿‡æœ€å¤§åŒ–ä¸åŒç±»åˆ«ä¹‹é—´çš„è·ç¦»ï¼ˆç±»é—´æ•£åº¦ï¼‰å’Œæœ€å°åŒ–åŒä¸€ç±»åˆ«å†…éƒ¨çš„æ–¹å·®ï¼ˆç±»å†…æ•£åº¦ï¼‰ï¼Œæ¥å®ç°åœ¨æ–°çš„ä½ç»´ç©ºé—´ä¸­ä½¿å¾—ä¸åŒç±»åˆ«æ›´å¥½åœ°å¯åˆ†çš„æŠ•å½±ã€‚</p>
<p>å°†æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼Œå…¶æ–°çš„å‡å€¼ä»¥åŠæ–¹å·®å¦‚ä¸‹ï¼š <span
class="math display">\[
\hat \mu=&amp;\frac 1 N\sum^N_{i=1}p^Tx_i=p^T\frac 1 N
\sum^N_{i=1}x_i=p^T\mu\\
\hat \sigma^2=&amp;\frac 1 N \sum^N_{i=1}(p^Tx_i-p^T\mu)^2\\
=&amp;\frac 1 N \sum^N_{i=1}p^T(x_i-\mu)(x_i-\mu)^Tp\\
=&amp;p^T(\frac 1 N \sum^N_{i=1}(x_i-\mu)(x_i-\mu)^T)p
\]</span> ç±»å†…(between)ä»¥åŠç±»é—´(within)æ•£åº¦: <span
class="math display">\[
S_b=(\mu_1-\mu_2)(\mu_1-\mu_2)^T\\
S_w=\sum_{j=1,2}\frac 1 {N_j}\sum^{N_j}_{i=1}(x_i-\mu)(x_i-\mu)^T
\]</span> å…¶ç›®æ ‡å‡½æ•°æ˜¯ï¼š <span class="math display">\[
\max \frac{p^TS_bp}{p^TS_wp}
\]</span> å°†ä¸Šè¿°å½¢å¼åŒ–ä¸ºæ‹‰æ ¼æœ—æ—¥å¯¹å¶é—®é¢˜çš„å½¢å¼ï¼š <span
class="math display">\[
\max p^TS_bp\\
s.t.\ p^TS_wp=1\\\\
\mathcal{L}=p^TS_bp-\lambda(p^TS_wp-1)
\]</span> å¯¹å…¶æ±‚åå¯¼å¾—ï¼š <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial p}=0\\
2S_bp-2\lambda S_wp=0\\
S_bp=\lambda S_wp\\
S_w^{-1}S_bp=\lambda p
\]</span> å¾—åˆ°çš„å½¢å¼åˆšå¥½æ˜¯æ±‚è§£ç‰¹å¾å‘é‡çš„æ ‡å‡†å½¢å¼(<span
class="math inline">\(Ax=\lambda x\)</span>)</p>
<p>At optimum, we have <span
class="math inline">\(p^{*^T}S_bp^*=\lambda\)</span></p>
<p>è¡¨ç¤ºåœ¨æœ€ä¼˜æ¡ä»¶ä¸‹ï¼ŒæŠ•å½±å‘é‡<span
class="math inline">\(p\)</span>çš„è½¬ç½®ä¸ç±»å†…æ•£åº¦çŸ©é˜µ<span
class="math inline">\(S_b\)</span>çš„ä¹˜ç§¯å†ä¸<span
class="math inline">\(p\)</span>ç›¸ä¹˜çš„ç»“æœç­‰äº<span
class="math inline">\(\lambda\)</span>ã€‚</p>
<p>è¿™ä¸ªæ–¹ç¨‹ç”¨äºç¡®å®šæœ€ä½³çš„æŠ•å½±å‘é‡<span
class="math inline">\(p\)</span>ï¼Œä½¿å¾—ç±»åˆ«ä¹‹é—´çš„å·®å¼‚æœ€å¤§åŒ–ï¼ŒåŒæ—¶ç±»å†…æ–¹å·®æœ€å°åŒ–ã€‚<span
class="math inline">\(S_b\)</span>æ˜¯ç±»é—´æ•£åº¦çŸ©é˜µï¼Œè¡¨ç¤ºä¸åŒç±»åˆ«ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ã€‚<span
class="math inline">\(\lambda\)</span>æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œè¡¨ç¤ºæŠ•å½±å‘é‡<span
class="math inline">\(p\)</span>åœ¨æœ€ä¼˜æ¡ä»¶ä¸‹çš„ç‰¹å¾å€¼ã€‚</p>
<p><strong>å¦‚æœ<span class="math inline">\(S_w\)</span>
ä¸æ˜¯å¯é€†çš„(invertible)çš„ï¼š</strong>å°±ä½¿ç”¨<span
class="math inline">\((S_w+\lambda I)^{-1}\)</span>(è¿™ä¸ªæ˜¯å¯é€†çš„)</p>
<p>å¦‚æœLDAä¸”Multi-class: <span class="math display">\[
S_b=\sum^C_{i=1,j=1}(\mu_i-\mu_j)(\mu_i-\mu_j)^T=\sum_i(\mu_i-\mu)(\mu_i-\mu)^T\\
S_w=\sum^C_{j=1}\sum_{i\in C_j}(x_i-\mu)(x_i-\mu)^T\\
\max \frac{trace(P^TS_bP)}{reace(P^TS_wP)}
\]</span> åœ¨<span
class="math inline">\(S_w^{-1}S_b\)</span>ä¸­é€‰æ‹©å‰Cä¸ªç‰¹å¾å‘é‡ï¼Œæœ€å¤šå¯ä»¥æœ‰Cä¸ªæŠ•å½±ï¼Œè¿™å–å†³ä¸çŸ©é˜µçš„ç§©ï¼š
<span class="math display">\[
rank(S_w^{-1}S_b)\le rank(S_b)\le C
\]</span></p>
<h2 id="ch7-clustering">Ch7 Clustering</h2>
<h3 id="unsupervised-learning-1">Unsupervised learning</h3>
<p>Learning without supervision</p>
<ul>
<li>Find the distribution of data</li>
<li>Learning to generate samples</li>
<li>Clustering</li>
<li>Anomaly detection</li>
<li>Feature learning</li>
</ul>
<p>Clustering</p>
<ul>
<li>One of the most important unsupervised learning tasks</li>
<li>Clustering is the process of identifying groups, or clusters, of
data points in a (usually) multidimensional space.</li>
<li>Connection to distribution modeling: related to mixture model</li>
</ul>
<p>Clustering: discover groups such that samples within a group are more
similar to each other than samples across groups.</p>
<p>Clusteringçš„Applicationï¼š</p>
<ul>
<li>Segmentation</li>
<li>Superpixel</li>
<li>Vector quantization in Bag-of-feature model</li>
</ul>
<p>Clusteringçš„Ingredient:</p>
<ul>
<li>A dissimilarity function between samples</li>
<li>A loss function to evaluate clusters</li>
<li>Algorithm that optimizes this loss function</li>
</ul>
<p>è¡¡é‡dissimilarity function: <span class="math display">\[
D(x_i,X_i&#39;)=\sqrt{\sum^p_{i=1}(x_{ij}-x_{i&#39;j})^2}
\]</span> <span class="math inline">\(x_{ij}\)</span>: <span
class="math inline">\(x_i\)</span>çš„ç‰¹å¾ç‚¹ï¼Œj=1,2,<span
class="math inline">\(\dots\)</span>,p</p>
<p>Clustering is invariant to rotation and translation of features,
<strong>but not to scaling</strong>.</p>
<h3 id="k-means-clustering">K-means Clustering</h3>
<p>Basic idea:</p>
<ul>
<li>Each sample can only fall into one of the k groups</li>
<li>From each group, we can calculate the mean vectors, that is, the
average of samples falling into a group</li>
<li>Any sample should be closest to the mean vector of its own group
than the mean vectors of other groups</li>
</ul>
<p>K-means take an <strong>iterative</strong> approach for solving the
problem.</p>
<p><strong>Algorithm</strong>:</p>
<ul>
<li>E-step: fixed the current mean vectors of each group. If a sample is
closest to the i-th mean vector, then the sample is assigned to the i-th
group</li>
<li>M-step: fixed the assignment, calculate the mean vector of each
group</li>
<li>Iterate E step and M step, until converge</li>
</ul>
<p><strong>Questions</strong>:</p>
<p>Why using those two steps can lead to converged result?</p>
<blockquote>
<p>é€šè¿‡åå¤è¿­ä»£E-stepså’ŒM-steps,
K-meansç®—æ³•ä¼šé€æ¸ä¼˜åŒ–ç°‡åˆ†é…å’Œè´¨å¿ƒçš„ä½ç½®ï¼Œç›´åˆ°è¾¾åˆ°æ”¶æ•›çŠ¶æ€ã€‚åœ¨æ”¶æ•›çŠ¶æ€ä¸‹ï¼Œç°‡åˆ†é…ä¸å†å‘ç”Ÿå˜åŒ–ï¼Œè´¨å¿ƒçš„ä½ç½®ä¹Ÿç›¸å¯¹ç¨³å®šã€‚è¿™æ„å‘³ç€ç®—æ³•å·²ç»æ‰¾åˆ°äº†ä¸€ä¸ªç›¸å¯¹ç¨³å®šçš„èšç±»ç»“æœï¼Œè¾¾åˆ°äº†æ”¶æ•›çŠ¶æ€ã€‚</p>
</blockquote>
<p>Why always calculate mean vectors?</p>
<blockquote>
<p>ä¸ºäº†æ›´æ–°ç°‡çš„ä½ç½®ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è®¡ç®—æ•°æ®ç‚¹ä¸è´¨å¿ƒä¹‹é—´çš„è·ç¦»ï¼Œä»è€Œè¿›è¡Œç°‡åˆ†é…ã€‚</p>
</blockquote>
<p>What is the objective function of k-means clustering algorithm?</p>
<p>â€“ Objective function will give a measurement of the goodness of
clustering results</p>
<blockquote>
<p><span class="math display">\[
\mathcal{J} = \sum_{i=1}^{n} \sum_{i=1}^K r_{ik} ||x_i - Î¼_k||^2_2
\]</span></p>
<p><span class="math inline">\(r_{ik}\)</span>: Each point will be
assigned to one of the prototypes, the distance between the point to the
prototype is the cost of the assignment. We are seeking the optimal
assignment that can minimize the cost.</p>
<p><span class="math inline">\(\mu_k\)</span>: We are also seeking the
optimal prototypes that can minimize the total cost.</p>
</blockquote>
<p>Choose K? <strong>Cross validation</strong></p>
<p><strong>Limitations of k-means</strong></p>
<ol type="1">
<li>Hard assignments of data points to clusters can cause a small
perturbation to a data point to flip it to another cluster</li>
<li>Assumes spherical clusters and equal probabilities for each
cluster</li>
<li>Those limitations can be solved by GMM based clustering</li>
</ol>
<h3 id="gaussian-mixture-modelsgmm">Gaussian Mixture Models(GMM)</h3>
<p>æ··åˆé«˜æ–¯æ¨¡å‹ï¼šGMM can characterize the distribution that contains K
clusters.
GMMå‡è®¾è§‚å¯Ÿçš„æ•°æ®ç‚¹æ˜¯ä»å¤šä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ··åˆä¸­ç”Ÿæˆçš„ï¼Œæ¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒæœ‰ä¸€ä¸ª<span
class="math inline">\(\pi_k\)</span>(weight)ï¼Œè¡¨ç¤ºé€‰æ‹©è¿™ç§åˆ†å¸ƒçš„å¯èƒ½æ€§ã€‚
<span class="math display">\[
å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼šP(x)=\mathcal{N}(x|\mu_k,\Sigma_k)=\frac{exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1})(x-\mu)}{\sqrt{(2\pi)^k|\Sigma|}}\\
é«˜æ–¯æ··åˆæ¨¡å‹ï¼šP(x)=\sum^K_{k=1}\pi_k\mathcal{N}(x|\mu_k,\Sigma_k),\sum^K_{k=1}\pi_k=1,0\le\pi_k\le1
\]</span> Generative process:</p>
<ol type="1">
<li><p>Randomly select the k-th Gaussian distribution according to <span
class="math inline">\(\pi_k\)</span></p></li>
<li><p>Randomly sample x from the selected Gaussian</p>
<p>distribution</p></li>
</ol>
<p><span class="math display">\[
P(\gamma=k)=\pi_k\\
P(x|\gamma=k)=\mathcal{N}(x|\mu_k,\sum_k)\\
P(x)=\sum^K_{k=1}\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span></p>
<p><span class="math inline">\(\gamma\)</span>: latent variable,
æŒ‡æ¯ä¸ªæ•°æ®ç‚¹çš„æ½œåœ¨æˆåˆ†æˆ–ç±»åˆ«æ ‡ç­¾ï¼Œå…¶å®é™…å€¼æ˜¯æœªè§‚æµ‹åˆ°çš„ã€‚GMMå‡è®¾è§‚æµ‹æ•°æ®æ˜¯ç”±å¤šä¸ªé«˜æ–¯åˆ†å¸ƒç»„æˆçš„æ··åˆä½“ï¼Œæ¯ä¸ªé«˜æ–¯åˆ†å¸ƒå¯¹åº”ä¸€ä¸ªç±»åˆ«æˆ–æˆåˆ†ã€‚<span
class="math inline">\(\gamma\)</span>è¡¨ç¤ºæ¯ä¸ªæ•°æ®ç‚¹å±äºå“ªä¸ªæˆåˆ†æˆ–ç±»åˆ«ã€‚</p>
<p><strong>Latent Variable</strong>ï¼š</p>
<ol type="1">
<li>Intermediate results inside a generative process</li>
<li>Each sample is associated with a latent variable</li>
<li>We do not know its exact value</li>
<li>But we can infer how likely it can be</li>
</ol>
<p>å¯¹äºéšå˜é‡ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è§‚å¯Ÿæˆ–æµ‹é‡å®ƒï¼Œä½†æ˜¯å®ƒå¯¹æœ€ç»ˆçš„ç»“æœæœ‰å½±å“ã€‚åªèƒ½æ¨æµ‹å…¶å¯èƒ½æ€§ã€‚</p>
<blockquote>
<p>æ¯”å¦‚å­¦ç”Ÿçš„å­¦ä¹ èƒ½åŠ›ï¼Œæ˜¯latent
variableï¼Œä¼šå¯¹è€ƒè¯•ç»“æœäº§ç”Ÿå½±å“ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸èƒ½ç¡®åˆ‡åœ°è¡¡é‡å­¦ç”Ÿåœ°å­¦ä¹ èƒ½åŠ›ã€‚</p>
</blockquote>
<p>parameter estimation for GMM: <strong>use
MLE(æœ€å¤§ä¼¼ç„¶ä¼°è®¡)ï¼Œç„¶åæ±‚è§£åå¯¼<span
class="math inline">\(\frac{\partial \mathcal{L}}{\partial
\theta}=0\)</span></strong></p>
<p>åœ¨GMMä¸­ï¼Œä¼°è®¡æ¨¡å‹å‚æ•°é€šå¸¸é‡‡ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ˆExpectation-Maximizationï¼ŒEMç®—æ³•ï¼‰ã€‚EMç®—æ³•é€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œäº¤æ›¿è¿›è¡ŒEæ­¥å’ŒMæ­¥ã€‚</p>
<p>EM algorithm solution to GMM</p>
<ul>
<li><p>E-step: calculate the posterior probability about the latent
variable (for each <span class="math inline">\(x_i\)</span> calc prob
<span class="math inline">\(râ€™_{ik}\)</span> for each cluster, this
corresponds to assigning <span class="math inline">\(x_i\)</span> to a
cluster in
k-means)--æ ¹æ®å½“å‰çš„æ¨¡å‹å‚æ•°ï¼Œè®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹å±äºæ¯ä¸ªæˆåˆ†çš„åéªŒæ¦‚ç‡ï¼ˆå³ç»™å®šæ•°æ®ç‚¹çš„è§‚æµ‹å€¼å’Œæ¨¡å‹å‚æ•°ï¼Œè¯¥æ•°æ®ç‚¹å±äºæ¯ä¸ªæˆåˆ†çš„æ¦‚ç‡ï¼‰
<span class="math display">\[
r&#39;_{ik}=P(r_{ik}=1|x_i)=\frac{\mathcal{N}(x_i|\mu_k,\Sigma_K)\pi_k}{\sum^K_{j=1}\mathcal{N}(x_i|\mu_j,\Sigma_j)\pi_j}
\]</span></p></li>
<li><p>M-step: estimate parameters based the expectation of latent
variables (recalc GMM based on estimate in step E, in k-means this is
recalc the
centroids)--ä½¿ç”¨è¿™äº›åéªŒæ¦‚ç‡æ›´æ–°æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬æˆåˆ†çš„å‡å€¼ã€åæ–¹å·®çŸ©é˜µå’Œæ¯ä¸ªæˆåˆ†çš„æƒé‡ã€‚
<span class="math display">\[
\pi_k=\frac{\sum_i r&#39;_{ik}}{N}\\
\mu_k=\frac{\sum_ir&#39;_{ik}x_i}{\sum_ir&#39;_{ik}}\\
\Sigma_K=\frac{\sum_ir&#39;_{ik}(x_i-\mu_k)x_i-\mu_k)^T}{\sum_ir&#39;_{ik}}
\]</span></p></li>
</ul>
<p>EM algorithm (More general case)</p>
<ol type="1">
<li>The above solution represents a special case of a more general
method called EM-algorithm</li>
<li>It is widely used for learning the probabilistic models with latent
variable inside its generative process.</li>
<li>It iterates between an E-step and an M-step.<br />
</li>
<li>We can theoretically prove that each step will lead to a nonincrease
of loss function</li>
<li>The iterative process will converge to a local minimum</li>
</ol>
<p>å…¶é€šç”¨å½¢å¼å¦‚ä¸‹:</p>
<ul>
<li><p>E-step:è®¡ç®—éšå˜é‡çš„åéªŒæ¦‚ç‡:<span
class="math inline">\(P(z|x,\theta^t)\)</span>, <span
class="math inline">\(\theta^t\)</span>æ˜¯ç›®å‰æ¨¡å‹çš„å‚æ•°</p></li>
<li><p>M-step:åŸºäºæœŸæœ›çš„ä¼¼ç„¶ä¼°è®¡æ›´æ–°æ¨¡å‹å‚æ•° <span
class="math display">\[
\theta^{t+1}=\mathop{\arg\max}\limits_{\theta}E_{z|x,\theta}\log
P(z,x|\theta)=\int P(z|x,\theta^t)\log(P(x|\theta,z)P(z|theta))dz
\]</span></p></li>
</ul>
<p><strong>ä¸K-means çš„ connection</strong>ï¼š</p>
<ul>
<li>E-step in GMM is a soft version of K-means. <span
class="math inline">\(r_{ik}\in[0,1]\)</span>, instead of <span
class="math inline">\(\{0,1\}\)</span>.</li>
<li>M-step in GMM estimates the probabilities and the covariance matrix
of each cluster in addition to the means.</li>
<li>All <span class="math inline">\(\pi_k\)</span> are equal. <span
class="math inline">\(\sum_k=\delta^2I\)</span>, as <span
class="math inline">\(\delta^2\rightarrow0,r_{ik}\rightarrow\{0,1\}\)</span>,
and the 2 methods coincide.</li>
</ul>
<h2 id="ch8-kernel-method">Ch8 Kernel Method</h2>
<p>Kernel functionçš„æ€§è´¨ï¼ˆè¯æ˜Mercerå®šç†ï¼‰ï¼š</p>
<ul>
<li>positive semi-definite(åŠæ­£å®š)ï¼š<span
class="math inline">\(c^Tkc\ge0\)</span></li>
</ul>
<blockquote>
<p>Mercerå®šç†ï¼šå¦‚æœK(x,
y)æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ï¼Œé‚£ä¹ˆå®ƒå¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå†…ç§¯ç©ºé—´ä¸­çš„æ­£å®šæ ¸å‡½æ•°ï¼Œå³å­˜åœ¨ä¸€ä¸ªç‰¹å¾ç©ºé—´ï¼ˆç§°ä¸ºå¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ˜ å°„Ï•(x)å°†æ•°æ®æ˜ å°„åˆ°è¯¥ç©ºé—´ä¸­ï¼Œä½¿å¾—K(x,
y)å¯ä»¥è¡¨ç¤ºä¸ºå†…ç§¯çš„å½¢å¼ã€‚</p>
<p>å¦‚æœå¯¹äºä»»æ„çš„nä¸ªå®æ•°<span class="math inline">\(a_1, a_2, ...,
a_n\)</span>å’ŒÎ©ä¸­çš„nä¸ªç‚¹<span class="math inline">\(x_1, x_2, ...,
x_n\)</span>ï¼ŒçŸ©é˜µ<span class="math inline">\(K = [K(x_i,
x_j)]\)</span>æ˜¯å¯¹ç§°çš„ï¼Œå¹¶ä¸”å¯¹äºä»»æ„éé›¶çš„å‘é‡<span
class="math inline">\(c = (c_1, c_2, ..., c_n)\)</span>ï¼Œæœ‰<span
class="math inline">\(âˆ‘âˆ‘c_ic_jK(x_i, x_j) â‰¥ 0\)</span>ï¼Œé‚£ä¹ˆK(x,
y)æ˜¯ä¸€ä¸ªåŠæ­£å®šæ ¸å‡½æ•°ã€‚</p>
</blockquote>
<h4 id="polynomial-kernel">Polynomial Kernel</h4>
<p>å¤šæ¬¡é¡¹æ ¸å‡½æ•°ï¼Œæ¯”è¾ƒå¸¸ç”¨ã€‚å…¶å½¢å¼å¦‚ä¸‹ï¼š <span class="math display">\[
K(x_i,x_j)=(x_i\cdot x_j+c)^d
\]</span></p>
<h4 id="radial-basis-function-kernelé«˜æ–¯æ ¸">Radial Basis Function
Kernel(é«˜æ–¯æ ¸)</h4>
<p>æ›´å¸¸ç”¨çš„æ ¸å‡½æ•°ï¼Œå…¶å½¢å¼å¦‚ä¸‹ï¼š <span class="math display">\[
K(x_i,x_j)=e^{-\gamma(x_i-x_j)^2}\\
=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}
\]</span></p>
<h3 id="kernel-in-svm">Kernel in SVM</h3>
<p>æˆ‘ä»¬åŸå§‹çš„å¯¹å¶é—®é¢˜æ˜¯ï¼š <span class="math display">\[
\mathcal{L}=\sum\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_ix_j
\]</span> æ–°çš„å¯¹å¶é—®é¢˜å®é™…ä¸Šæ˜¯æŠŠ<span
class="math inline">\(x\)</span>æ¢æˆäº†<span
class="math inline">\(\phi(x)\)</span>(å‡ç»´): <span
class="math display">\[
\mathcal{L}=\sum\alpha_i-\frac 1 2
\sum_i\sum_j\alpha_i\alpha_jy_iy_j\phi(x_i)\phi(x_j)
\]</span>
æˆ‘ä»¬é€‰å®šæ ¸å‡½æ•°ï¼š<em>åŸå…ˆçš„å‡ç»´å‡½æ•°ä¸å®œæ±‚è§£ï¼Œä½†æ ¸å‡½æ•°å¯ä»¥æ±‚è§£</em> <span
class="math display">\[
k(x_i,x_j)=\phi(x_i)\cdot\phi(x_j)
\]</span></p>
<blockquote>
<ul>
<li>We stop caring about what the transform represents</li>
<li>We start caring about how well it classifies our data</li>
</ul>
</blockquote>
<p>å…¶ä»–è¯¦ç»†åœ°æ¨å¯¼æ²¡æœ‰ï¼Œä¸»è¦æ˜¯åœ¨CVXä¸­çš„åº”ç”¨ã€‚</p>
<h3 id="kernel-in-k-means">Kernel in K-means</h3>
<p>Map the data space into some kernel space.</p>
<p>Hope this new space better allows the data to be separated!</p>
<p>ä¸‹é¢æ˜¯K-meansçš„è¿‡ç¨‹ï¼š</p>
<p>1.Assigning Points to Clusters</p>
<p>(åŸæ¥çš„) <span class="math display">\[
assign(x_i)=\mathop{\arg\min}\limits_{k}||x_i-\mu_k||^2\\
=(x_i-\mu_k)^T(x_i-\mu_k)
\]</span> ï¼ˆå°†<span class="math inline">\(x_i\)</span>æ¢æˆäº†<span
class="math inline">\(\phi(x_i)\)</span>çš„ï¼‰ <span
class="math display">\[
assign(x_i)=\mathop{\arg\min}\limits_{k}||\phi(x_i)-\mu_k||^2\\
=(\phi(x_i)-\mu_k)^T(\phi(x_i)-\mu_k)
\]</span> 2.Defining Cluster Mid-Points</p>
<p>(åŸæ¥çš„) <span class="math display">\[
\mu_k=\frac 1 {N_k}\sum^N_{i=1}z_{ik}x_i
\]</span> <span class="math inline">\(Z_{ik}=1\)</span>ï¼šæ ·æœ¬<span
class="math inline">\(x_i\)</span>å±äºkç±»ï¼Œ=0ä¸å±äºkç±»</p>
<p>ï¼ˆå°†<span class="math inline">\(x_i\)</span>æ¢æˆäº†<span
class="math inline">\(\phi(x_i)\)</span>çš„ï¼‰ <span
class="math display">\[
\mu_k=\frac {\sum_{i=1}z_{ik}\phi(x_i)}{\sum_{i=1}z_{ik}}
\]</span></p>
<h3 id="kernel-in-linear-regression">Kernel in Linear Regression</h3>
<p>åŸæ¥çš„ï¼š <span class="math display">\[
\sum_i||w^Tx_i-y_i||^2
\]</span> ç°åœ¨çš„ï¼š <span class="math display">\[
\sum_i||w^T\phi(x_i)-y_i||^2
\]</span> æˆ‘ä»¬ä»¤<span
class="math inline">\(w=\sum_j\alpha_jx_j+o\)</span>,ä»£å…¥ç›®æ ‡å‡½æ•°å¾—ï¼š
<span class="math display">\[
\sum_i||(\sum_j\alpha_jx_j+o)^Tx_i-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jx_i\cdot x_j+x_i\cdot o-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jK(x_i,x_j)+K(x_i,o)-y_i||^2_2\\
=\sum_i||\sum_j\alpha_jK(x_i,x_j)-y_i||^2_2
\]</span></p>
<blockquote>
<p>o: æ˜ å°„å‡½æ•°Ï†(x)å¯èƒ½å¸¦å…¥çš„ä¸€ä¸ªé¢å¤– bias term</p>
<p><span class="math inline">\(K(x_i,o)=0\)</span>:</p>
<ol type="1">
<li>oä¸ä»£è¡¨ä»»ä½•ä¸€ä¸ªå®é™…æ ·æœ¬ç‚¹ã€‚å®ƒä»…ä»…è¡¨ç¤ºå¼•å…¥æ ¸æŠ€å·§å¯èƒ½å¸¦æ¥çš„ä¸€ä¸ªé¢å¤–çš„biasé¡¹ã€‚</li>
<li>å› æ­¤,ä»»ä½•æ ·æœ¬ç‚¹<span
class="math inline">\(x_i\)</span>ä¸oçš„è·ç¦»<span
class="math inline">\(||x_i -
o||\)</span>éƒ½æ˜¯æ— å®šä¹‰çš„ã€‚æ— æ³•ç›´æ¥è®¡ç®—æ ¸å‡½æ•°å€¼ã€‚</li>
<li>å› æ­¤å°±ç›´æ¥å®šä¹‰=0</li>
</ol>
</blockquote>
<h3 id="kernel-in-pca">Kernel in PCA</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th>Tasks</th>
<th>Problems</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Replace x with <span class="math inline">\(\phi(x)\)</span></td>
<td>Centralising the data</td>
</tr>
<tr class="even">
<td>Turn everything into dot products</td>
<td>Calculating the covariance matrix</td>
</tr>
<tr class="odd">
<td>Use <span class="math inline">\(K(x_i , x_j )\)</span> a lot</td>
<td>Calculating the projection matrix</td>
</tr>
<tr class="even">
<td>Never use <span class="math inline">\(\phi(x)\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
Cov=X^TX\\
Cov(k,m)=\sum_i^nX_i^kX^m_i\\
=\sum^n_{i,j=1}c_ic_jk(x_ix_j)=c^TKc\ge0
\]</span></p>
<p><strong>è®¡ç®—åæ–¹å·®çŸ©é˜µ</strong>ï¼š <span class="math display">\[
\bar X\bar X^Tv=\lambda v\\
\]</span></p>
<blockquote>
<p><span class="math inline">\(\bar X\)</span>: centralised data
matrix(d x n)</p>
<p>d: å±æ€§çš„ç»´æ•°ï¼Œnï¼šæœ‰å¤šå°‘æ ·æœ¬</p>
<p><span class="math inline">\(\bar X\bar X^T\)</span>:(d x
d)åæ–¹å·®çŸ©é˜µ</p>
<p><span class="math inline">\(v\)</span>: åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡</p>
</blockquote>
<p>ç»è¿‡kernel matrix: <span class="math display">\[
\bar X\bar X^Tv&#39;=\lambda v&#39;\\
v=\bar Xv&#39;
\]</span> <strong>è®¡ç®—å»ä¸­å¿ƒåŒ–</strong>ï¼š <span class="math display">\[
\bar k(x_i,x_j)=(\varphi(x_i)-\mu)^T(c-\mu)\\
=(\varphi(x_i)-\frac 1 N\sum_k\varphi(x_k))^T(\varphi(x_j)-\frac 1
N\sum_k\varphi(x_k))\\
=\varphi(x_i)\cdot\varphi(x_j)-\frac 1
N\sum_k\varphi(x_i)\cdot\varphi(x_k)-\frac 1
N\sum_k\varphi(x_j)\cdot\varphi(x_k)-\frac 1
{N^2}\sum_k\varphi(x_i)\cdot\varphi(x_j)\\
=k(x_i,x_j)-\frac 1 N \sum_kk(x_i,x_k)-\frac 1 N \sum_kk(x_j,x_k)-\frac
1 {N^2} \sum_{ij}k(x_i,x_j)\\
K&#39;=K-1_NK-k1_N+1_NK1_N
\]</span></p>
<blockquote>
<p><span class="math inline">\(1_N\)</span> here represents a NxN matrix
filled with 1/N</p>
</blockquote>
<p><strong>è®¡ç®—æŠ•å½±çŸ©é˜µ</strong>ï¼šåŸæ¥æ˜¯<span
class="math inline">\(y=P^T(x-\mu)\)</span>,ç°åœ¨æ˜¯ï¼š <span
class="math display">\[
y_k=p_k^T(\varphi(x_t)-\mu)
\]</span> å¸¦å…¥<span class="math inline">\(v=\varphi(\bar
X)v&#39;\)</span>: <span class="math display">\[
y_k=v&#39;^T_k\varphi(\bar X)^T(\varphi(x_t)-\frac 1
N\sum_i\varphi(x_i))\\
=v&#39;^T_k(\varphi(X)-\frac 1 N\sum_i\varphi(x_i))^T(\varphi(x_t)-\frac
1 N\sum_i\varphi(x_i))
\]</span></p>
<h2 id="ch9-ensemble-method">Ch9 Ensemble Method</h2>
<p>When to use?</p>
<ul>
<li>Individual decisions are cheap, so making many of them is easy</li>
<li>Making an expert is expensive or impossible</li>
<li>Your amateurs are independent of each other</li>
</ul>
<h3 id="decision-tree-1">Decision Tree</h3>
<p>Entropy=<span class="math inline">\(\sum-p_i\log_2p_i\)</span>(Gini
Function), <span class="math inline">\(p_i\)</span>ä»£è¡¨å±äºclass i
çš„æ¦‚ç‡</p>
<p>Gini Coefficient of
Purityæ˜¯åˆ©ç”¨Giniä¸å‡è¡¡åº¦æ¥åº¦é‡åˆ†ç±»é—®é¢˜æ ·æœ¬é›†åˆçš„çº¯ç¨‹åº¦ã€‚å®ƒæœåˆ¤å®šæ¨¡å‹é¢„æµ‹æ€§èƒ½çš„ä¸€ä¸ªæŒ‡æ ‡(è¡¡é‡å†³ç­–æ ‘æ¯ä¸ªèŠ‚ç‚¹çš„çº¯åº¦):</p>
<ul>
<li>å®ƒé€šè¿‡è®¡ç®—æ ·æœ¬é›†åˆä¸­çš„ç±»åˆ†å¸ƒä¸å‡è¡¡ç¨‹åº¦,æ¥åæ˜ çº¯åº¦ç¨‹åº¦ã€‚</li>
<li>å¦‚æœæ ·æœ¬å…¨æ˜¯åŒä¸€ç±»,ç±»åˆ†å¸ƒæœ€å‡è¡¡,Giniå€¼ä¸º0,è¿™ä¹Ÿå°±æ˜¯100%çš„çº¯åº¦ã€‚</li>
<li>å¦åˆ™ä¸åŒç±»æ ·æœ¬æ··æ‚,Giniå€¼è¶Šé«˜,è¡¨ç¤ºçº¯åº¦è¶Šä½ã€‚</li>
</ul>
<p><strong>The Flaw of the Decision Tree</strong>:</p>
<ul>
<li>We have a series of decisions, but some of these decisions can only
be conceived of as overfitting.
ï¼ˆæ ¹æ®è®­ç»ƒæ•°æ®é›†é€’å½’åœ°æ„å»ºä¾èµ–äºæ•°æ®çš„å†³ç­–è¾¹ç•Œ,ä¸æ–­åœ°ç»†åˆ†è®­ç»ƒé›†æ ·æœ¬ï¼‰</li>
<li>There are numerous places we could place decision boundaries.
(ä¼šå¯¼è‡´å†³ç­–è¾¹ç•Œè¿‡äºå¤æ‚ï¼Œè®­ç»ƒé›†æ•ˆæœä¸é”™ï¼Œä½†æ˜¯æµ‹è¯•é›†çš„æ•ˆæœå¾ˆå·®)</li>
<li>How can we avoid these overfit decision
boundaries?--æ¯”å¦‚å‰ªæï¼Œæˆ–é™å®šæœ€å¤§æ ‘æ·±</li>
</ul>
<h3 id="random-forest">Random Forest</h3>
<p>å®šä¹‰ï¼šA random forest is a series of decision trees which are
randomized. You then apply your new measurement to each decision tree,
picking the aggregate decision by voting.</p>
<p>å®¹æ˜“å‡ºç°çš„é—®é¢˜ï¼š</p>
<ul>
<li>Some potential decision boundaries represent overfitting.</li>
<li>Some features are not representative of generalisation.</li>
</ul>
<p>æˆ‘ä»¬ä½¿ç”¨bootstrappingæ–¹æ³•ï¼ˆéšæœºæœ‰æ”¾å›åœ°é‡‡æ ·--random selection with
replacementï¼‰å¾—åˆ°å­æ•°æ®é›†ï¼ˆsubsetsï¼‰</p>
<p>å€¼å¾—è€ƒè™‘åœ°é—®é¢˜æ˜¯ï¼š</p>
<p>How many trees?</p>
<ul>
<li>Too few trees:
<ul>
<li>Might interpret random variation as signal.</li>
<li>Biased towards individual trees.</li>
</ul></li>
<li>Too many trees: Slow?</li>
</ul>
<p>How many dimensions/tree? --ä»å®è·µæ¥çœ‹ä¸€èˆ¬æ˜¯<span
class="math inline">\(\sqrt{D}\)</span>æˆ–æ˜¯<span
class="math inline">\(\log(D)\)</span></p>
<p><strong>Bagging doesnâ€™t choose a random subset of
dimensions</strong></p>
<blockquote>
<ul>
<li>Baggingç®—æ³•ä¸ºäº†å‡è½»è¿‡æ‹Ÿåˆ,å…¶ä¸»è¦æ€æƒ³æ˜¯ä»åŸå§‹è®­ç»ƒæ•°æ®é›†ä¸­é‡‡æ ·å‡ºå¤šä¸ªå¹¶è¡Œçš„å­æ•°æ®é›†,æ¥è®­ç»ƒå¤šä¸ªåŸºå­¦ä¹ å™¨æ¨¡å‹ã€‚(é‡‡æ ·è¿‡ç¨‹ä½¿ç”¨çš„æ˜¯boostrappingæ–¹æ³•)</li>
<li>ä½†åœ¨é‡‡æ ·å­æ•°æ®é›†æ—¶,Baggingé‡‡æ ·çš„æ˜¯æ ·æœ¬å®ä¾‹,è€Œä¸æ˜¯ç‰¹å¾ç»´åº¦ã€‚</li>
<li>ä¹Ÿå°±æ˜¯è¯´,æ¯ä¸ªå­æ•°æ®é›†ä¸­åŒ…å«çš„æ˜¯åŸå§‹æ‰€æœ‰ç‰¹å¾ç»´åº¦å¯¹åº”çš„ä¸€éƒ¨åˆ†æ ·æœ¬ã€‚è€Œä¸æ˜¯éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾ç»´åº¦ã€‚</li>
</ul>
</blockquote>
<h3 id="boosting">Boosting</h3>
<p>idea: We build a classifier, but it incorrectly classifies some
samples, we can further train this classifier to perform better on the
incorrectly labelled samples(ä»é”™è¯¯ä¸­å­¦ä¹ ï¼‰</p>
<ul>
<li>We make decisions based on a weighted decision of minimal â€˜weak
classifiersâ€™ (stumps)</li>
<li>After we have built our â€˜one stumpâ€™ we can judge how good it is, by
the number of instances it gets wrong.</li>
</ul>
<p><span class="math display">\[
\alpha=\frac1 2\ln(\frac{1-TotalError}{TotalError})
\]</span></p>
<p><span class="math inline">\(\alpha\)</span>ä»£è¡¨æ¯ä¸ªdecision tree
åœ¨æœ€ç»ˆçš„å†³ç­–ä¸­çš„è´¡çŒ®ç¨‹åº¦ã€‚æˆ‘ä»¬é€šè¿‡å®ƒæ›´æ–°æƒé‡ï¼š <span
class="math display">\[
w_{new}=w_{old}\times e^a
\]</span></p>
<h4 id="adaboost">Adaboost</h4>
<p>ç›®æ ‡å‡½æ•°ï¼š <span class="math display">\[
\min_{\{a_k,\theta_m\}}\sum_ie^{-y_ih_m(x_i)}\\
h_m(x)=\alpha_1h(x;\theta_1)+\dots+\alpha_mh(x;\theta_m)
\]</span> loss function: <span class="math display">\[
e^{-yh_m(x)}
\]</span></p>
<p>è¿˜æœ‰å…¶ä»–çš„boostçš„æ–¹æ³•ï¼Œæ¯”å¦‚XGboostï¼ŒGradient Boostã€‚</p>
<h4 id="æ·»åŠ weak-learner">æ·»åŠ weak learner</h4>
<p>ç¬¬<span class="math inline">\(m^{th}\)</span>è½®æ·»åŠ ï¼š <span
class="math display">\[
\sum^n_{i=1}e^{-y_i[h_{m-1}(x_i)+\alpha_mh(x_i;\theta_m)]}\\
=\sum^n_{i=1}e^{-y_ih_{m-1}(x_i)-y_i\alpha_mh(x_i;\theta_m)}
\]</span> å› ä¸º<span class="math inline">\(e^{-y_i
h_{m-1}(x_i)}=W_i^{m-1}\)</span>,ä¹Ÿå°±æ˜¯ä¸Šä¸€è½®çš„ç»“æœ(å¯¹wæ›´æ–°)ï¼š <span
class="math display">\[
W_i^m=\sum^n_{i=1}W_i^{m-1}e^{-y_i\alpha_mh(x_i;\theta_m)}
\]</span></p>
<p>å¯¹<span class="math inline">\(h_m\)</span>çš„æ›´æ–°ï¼š <span
class="math display">\[
h_m(x_i)=h_{m-1}+\alpha_mh(x_i;\theta_m)
\]</span></p>
<p>What do we have in common?</p>
<ul>
<li>We are weighting our bad decisions higher (i.e., boosting)</li>
<li>We are using ensemble learning</li>
</ul>
<h2 id="ch10-deep-learning">Ch10 Deep Learning</h2>
<p>å…³äºLossçš„ä»‹ç»ï¼š</p>
<ol type="1">
<li>Loss is some measure of the error between the predicted and target
values</li>
<li>A metric for determining how well a model has trained</li>
<li>Lower loss/cost/error =&gt; better model =&gt; better learned
parameters</li>
</ol>
<p>æˆ‘ä»¬å¸¸ç”¨çš„Lossæ˜¯MSE(Mean Squared Error): <span
class="math display">\[
MSE(X,h_{\theta})=\frac1 m\sum^m_{i=1}(\theta^Tx_i-y_i)^2
\]</span></p>
<p>å…¨å±€æœ€å°å€¼ä»¥åŠå±€éƒ¨æœ€å°å€¼ï¼šGlobal Minimum is the smallest error out of
all possible parameter configurations, Local Minimum is the smallest
error in some region of space.</p>
<p>ä¸€èˆ¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™(Gradient Descent)æ¥æ›´æ–°å‚æ•°ï¼š <span
class="math display">\[
\theta_{n+1}=\theta_n-\eta\nabla_{\theta}MSE(\theta_n)
\]</span></p>
<h3 id="learning-rateeta">Learning Rate(<span
class="math inline">\(\eta\)</span>)</h3>
<p><span class="math inline">\(\eta\)</span> is a Hyperparameter!</p>
<p>Choice of learning rate is very important:</p>
<ul>
<li>Too smallâ€¦ takes forever to train, can easily get stuck in tiny
local minima</li>
<li>Too bigâ€¦ can overstep or become unstable</li>
</ul>
<p><strong>Activation
Function</strong>ï¼šå¸¸ç”¨çš„æœ‰sigmoidï¼Œreluï¼ŒTanhå‡½æ•°</p>
<p>Softmax:å°†ç»“æœè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ <span class="math display">\[
softmax:\frac{exp(s_k(x))}{\sum^K_{i=1}exp(s_i(x))}\\
cost\ function:\mathcal{J}(\theta)=-\frac 1 m
\sum^m_{i=1}\sum^K_{k=1}y_k^i\log(\hat p_k^i)
\]</span></p>
<h3 id="neural-networks">Neural Networks</h3>
<p>Terms:</p>
<ul>
<li>Iterations (one per â€˜batch)</li>
<li>Epochs (one per â€˜all samplesâ€™)</li>
<li>Batches(å¯¹äºå›¾ç‰‡æ¥è¯´å°±æ˜¯mega-pixels)</li>
</ul>
<h3 id="cnn">CNN</h3>
<p>å…¶æ­¥éª¤å¦‚ä¸‹ï¼š</p>
<p>1.å·ç§¯å±‚(Convolutional Layer)--åˆ©ç”¨paddingè¡¥å…¨</p>
<ul>
<li>ä½¿ç”¨å·ç§¯æ“ä½œæå–å›¾åƒä¸­çš„ç‰¹å¾,å¦‚è¾¹ç¼˜ã€è‰²å½©ç­‰ã€‚</li>
<li>å‚æ•°åŒ…æ‹¬è¿‡æ»¤å™¨(kernel)å’Œåç½®ã€‚</li>
</ul>
<p>2.æ± åŒ–å±‚(Pooling Layer)</p>
<ul>
<li>å¯¹ç‰¹å¾å›¾åŒºåŸŸè¿›è¡Œä¸‹é‡‡æ ·,é™ä½ç©ºé—´ç»´åº¦,å¢åŠ ç¿»è½¬ä¸å˜æ€§å’Œæ§åˆ¶è¿‡æ‹Ÿåˆã€‚</li>
<li>å¸¸ç”¨æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–ã€‚</li>
</ul>
<p>3.æ¿€æ´»å±‚(Activation Layer)</p>
<ul>
<li>ä¸ºå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚æ·»åŠ éçº¿æ€§æœºèƒ½,å¦‚ReLUã€Sigmoidã€Tanhç­‰ã€‚</li>
</ul>
<ol type="1">
<li>å…¨è¿æ¥å±‚(Fully Connected Layer)</li>
</ol>
<ul>
<li>å°†ä¸Šä¸€å±‚ç‰¹å¾æ•´åˆæˆä¸€ç»´æ•°ç»„,ä¸è¯¥å±‚æƒå€¼ç›¸ä¹˜å®ç°åˆ†ç±»æˆ–å›å½’åŠŸèƒ½ã€‚</li>
</ul>
<p>4.Softmaxå±‚</p>
<ul>
<li>å°†å…¨è¿æ¥å±‚è¾“å‡ºè¿›è¡ŒSoftmaxå½’ä¸€åŒ–,è¾“å‡ºæ¦‚ç‡è¯„åˆ†å®ç°åˆ†ç±»ã€‚</li>
</ul>
<h2 id="è€ƒç‚¹æ•´ç†">è€ƒç‚¹æ•´ç†</h2>
<p>cross-validation:</p>
<p>æˆ‘ä»¬å­¦ä¹ çš„ä¸åŒæ–¹æ³•æ˜¯å±äºæœ‰ç›‘ç£å­¦ä¹ è¿˜æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Ÿ</p>
<p>LDA,PCA,GMM,SVRç­‰ç­‰</p>
<p>instance/generative/discriminativeçš„åŒºåˆ«ä»¥åŠåº”ç”¨</p>
<p>SVMçš„å…¬å¼æ¨å¯¼ï¼Œå„ä¸ªæ¦‚å¿µçš„å®šä¹‰ï¼šmargin,soft/hard-margin,dual/primal
problem</p>
<p>é›†æˆå­¦ä¹ ä¸­çš„æ¦‚å¿µã€å›å½’æ¨¡å‹çš„ç‰¹ç‚¹</p>
<p>K-means,GMMæ¦‚å¿µ</p>
<p>PCAçš„æ¦‚å¿µä»¥åŠè¿‡ç¨‹ï¼ŒLDAæ¦‚å¿µï¼Œæ ¸å‡½æ•°çš„è¯æ˜</p>
<p>æœ€åæœ‰å…³CNNçš„è®¡ç®—ï¼šæ•´ä½“çš„è¿‡ç¨‹ä»¥åŠå‚æ•°çš„ä¸ªæ•°</p>
</div><div class="post-end"><div class="post-prev"><a href="/2024/12/01/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90-Ch1-%E5%A4%9A%E5%85%83%E5%88%86%E5%B8%83/" title="ä¸Šä¸€ç¯‡æ–‡ç« "><i class="fa-solid fa-chevron-left fa-lg"></i></a></div><div class="post-next"><a href="/2023/11/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Ch7-10/" title="ä¸‹ä¸€ç¯‡æ–‡ç« "><i class="fa-solid fa-chevron-right fa-lg"></i></a></div></div></article><div class="comment" id="comment"><script src="https://giscus.app/client.js" data-repo="SchwertLin/SwertLin_Blog_Comment" data-repo-id="R_kgDONXjrCQ" data-category="Announcements" data-category-id="DIC_kwDONXjrCc4Cky9X" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" crossorigin="anonymous" async="async"></script></div><div id="post-toc"><aside class="toc-aside"><div class="toc-title"><span><i class="fa-solid fa-paw"></i>ç›®å½•</span></div><div class="toc-container" id="toc-body"><ol class="toc-content"><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#contents"><span class="toc-content-number">1.</span> <span class="toc-content-text">Contents</span></a></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch1-overview-of-ml"><span class="toc-content-number">2.</span> <span class="toc-content-text">Ch1 Overview of ML</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#supervised-learning"><span class="toc-content-number">2.1.</span> <span class="toc-content-text">Supervised learning</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#unsupervised-learning"><span class="toc-content-number">2.2.</span> <span class="toc-content-text">Unsupervised learning</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="toc-content-number">2.3.</span> <span class="toc-content-text">æ•°å­¦åŸºç¡€</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-content-number">2.3.1.</span> <span class="toc-content-text">çº¿æ€§ä»£æ•°</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC"><span class="toc-content-number">2.3.2.</span> <span class="toc-content-text">çŸ©é˜µæ±‚å¯¼</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1"><span class="toc-content-number">2.3.3.</span> <span class="toc-content-text">æ¦‚ç‡ä¸ç»Ÿè®¡</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E7%A7%8D%E7%B1%BB"><span class="toc-content-number">2.4.</span> <span class="toc-content-text">ä¼˜åŒ–é—®é¢˜çš„ç§ç±»</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch2-classification"><span class="toc-content-number">3.</span> <span class="toc-content-text">Ch2 Classification</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#knn"><span class="toc-content-number">3.1.</span> <span class="toc-content-text">KNN</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#overfitting%E4%B8%8Eunderfitting"><span class="toc-content-number">3.1.1.</span> <span class="toc-content-text">overfittingä¸underfitting</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#validation-set"><span class="toc-content-number">3.1.2.</span> <span class="toc-content-text">Validation set</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#naive-bayes"><span class="toc-content-number">3.2.</span> <span class="toc-content-text">Naive Bayes</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#application"><span class="toc-content-number">3.2.1.</span> <span class="toc-content-text">Application</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#decision-tree"><span class="toc-content-number">3.3.</span> <span class="toc-content-text">Decision Tree</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch3-regression"><span class="toc-content-number">4.</span> <span class="toc-content-text">Ch3 Regression</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#linear-regression"><span class="toc-content-number">4.1.</span> <span class="toc-content-text">Linear Regression</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#regularized-regression"><span class="toc-content-number">4.2.</span> <span class="toc-content-text">Regularized Regression</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#ridge-regression"><span class="toc-content-number">4.2.1.</span> <span class="toc-content-text">Ridge Regression</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#lasso-regression"><span class="toc-content-number">4.2.2.</span> <span class="toc-content-text">Lasso Regression</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#support-vector-regression"><span class="toc-content-number">4.3.</span> <span class="toc-content-text">Support Vector Regression</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#dual-form-of-svr"><span class="toc-content-number">4.3.1.</span> <span class="toc-content-text">Dual form of SVR</span></a></li></ol></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch45-linear-classification-and-svm"><span class="toc-content-number">5.</span> <span class="toc-content-text">Ch4&amp;5 Linear
Classification and SVM</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#linear-classifiers"><span class="toc-content-number">5.1.</span> <span class="toc-content-text">Linear Classifiers</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#convex-theorem"><span class="toc-content-number">5.2.</span> <span class="toc-content-text">Convex Theorem</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#svm"><span class="toc-content-number">5.3.</span> <span class="toc-content-text">SVM</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#hard-margin"><span class="toc-content-number">5.3.1.</span> <span class="toc-content-text">Hard-margin</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#soft-margin"><span class="toc-content-number">5.3.2.</span> <span class="toc-content-text">Soft-margin</span></a></li></ol></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch6-pca-lda-and-dimensionality-reduction"><span class="toc-content-number">6.</span> <span class="toc-content-text">Ch6 PCA, LDA and
Dimensionality reduction</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#dimensionality-reduction"><span class="toc-content-number">6.1.</span> <span class="toc-content-text">Dimensionality reduction</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#pcaprincipal-component-analysis"><span class="toc-content-number">6.2.</span> <span class="toc-content-text">PCA(Principal Component
Analysis)</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#ldalinear-discriminative-analysis"><span class="toc-content-number">6.3.</span> <span class="toc-content-text">LDA(Linear Discriminative
Analysis)</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch7-clustering"><span class="toc-content-number">7.</span> <span class="toc-content-text">Ch7 Clustering</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#unsupervised-learning-1"><span class="toc-content-number">7.1.</span> <span class="toc-content-text">Unsupervised learning</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#k-means-clustering"><span class="toc-content-number">7.2.</span> <span class="toc-content-text">K-means Clustering</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#gaussian-mixture-modelsgmm"><span class="toc-content-number">7.3.</span> <span class="toc-content-text">Gaussian Mixture Models(GMM)</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch8-kernel-method"><span class="toc-content-number">8.</span> <span class="toc-content-text">Ch8 Kernel Method</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#polynomial-kernel"><span class="toc-content-number">8.0.1.</span> <span class="toc-content-text">Polynomial Kernel</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#radial-basis-function-kernel%E9%AB%98%E6%96%AF%E6%A0%B8"><span class="toc-content-number">8.0.2.</span> <span class="toc-content-text">Radial Basis Function
Kernel(é«˜æ–¯æ ¸)</span></a></li></ol></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-svm"><span class="toc-content-number">8.1.</span> <span class="toc-content-text">Kernel in SVM</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-k-means"><span class="toc-content-number">8.2.</span> <span class="toc-content-text">Kernel in K-means</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-linear-regression"><span class="toc-content-number">8.3.</span> <span class="toc-content-text">Kernel in Linear Regression</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#kernel-in-pca"><span class="toc-content-number">8.4.</span> <span class="toc-content-text">Kernel in PCA</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch9-ensemble-method"><span class="toc-content-number">9.</span> <span class="toc-content-text">Ch9 Ensemble Method</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#decision-tree-1"><span class="toc-content-number">9.1.</span> <span class="toc-content-text">Decision Tree</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#random-forest"><span class="toc-content-number">9.2.</span> <span class="toc-content-text">Random Forest</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#boosting"><span class="toc-content-number">9.3.</span> <span class="toc-content-text">Boosting</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#adaboost"><span class="toc-content-number">9.3.1.</span> <span class="toc-content-text">Adaboost</span></a></li><li class="toc-content-item toc-content-level-4"><a class="toc-content-link" href="#%E6%B7%BB%E5%8A%A0weak-learner"><span class="toc-content-number">9.3.2.</span> <span class="toc-content-text">æ·»åŠ weak learner</span></a></li></ol></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#ch10-deep-learning"><span class="toc-content-number">10.</span> <span class="toc-content-text">Ch10 Deep Learning</span></a><ol class="toc-content-child"><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#learning-rateeta"><span class="toc-content-number">10.1.</span> <span class="toc-content-text">Learning Rate(\(\eta\))</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#neural-networks"><span class="toc-content-number">10.2.</span> <span class="toc-content-text">Neural Networks</span></a></li><li class="toc-content-item toc-content-level-3"><a class="toc-content-link" href="#cnn"><span class="toc-content-number">10.3.</span> <span class="toc-content-text">CNN</span></a></li></ol></li><li class="toc-content-item toc-content-level-2"><a class="toc-content-link" href="#%E8%80%83%E7%82%B9%E6%95%B4%E7%90%86"><span class="toc-content-number">11.</span> <span class="toc-content-text">è€ƒç‚¹æ•´ç†</span></a></li></ol></div></aside><div class="toc-blank" onclick="tocToggle()"></div></div><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async="async"></script></div></div><div id="tool-bar"><div id="tool-bar-main"><div id="tool-toggle" onclick="toolToggle()" title="è®¾ç½®"><i class="fa-solid fa-gear"></i></div><div id="toc-toggle" onclick="tocToggle()" title="ç›®å½•"><i class="fa-solid fa-list-ul"></i></div><div id="go-to-comment" onclick="gotoComment()" title="è¯„è®º"><i class="fa-regular fa-message fa-flip-horizontal"></i></div><div id="back-to-top" onclick="scrollToTop()" title="è¿”å›é¡¶éƒ¨"><i class="fa-solid fa-chevron-up"></i></div></div><div id="tool-bar-more" style="display: none;"><div id="darkmode-switch" onclick="darkmodeSwitch()" title="æ·±è‰²æ¨¡å¼"><i class="fa-solid fa-circle-half-stroke"></i></div><div id="font-size-increase" onclick="fontSizeIncrease()" title="æ”¾å¤§å­—ä½“"><i class="fa-solid fa-plus"></i></div><div id="font-size-decrease" onclick="fontSizeDecrease()" title="ç¼©å°å­—ä½“"><i class="fa-solid fa-minus"></i></div></div></div><div id="search-panel"><div class="search-container"><div class="search-head"><div class="search-title"><span><i class="fa-solid fa-paw"></i>æœç´¢</span></div><div class="search-close-btn" onclick="toggleSearchWindow()"><i class="fa-regular fa-circle-xmark"></i></div></div><div class="search-box"><i class="fa-solid fa-magnifying-glass"></i><input id="search-input" type="text" placeholder="è¯·è¾“å…¥éœ€è¦æœç´¢çš„å†…å®¹â€¦â€¦" value=""/></div><div class="search-body"><div id="search-count">åŒ¹é…ç»“æœæ•°: </div><div id="search-result"></div><div id="search-result-empty">æœªæœç´¢åˆ°åŒ¹é…çš„æ–‡ç« ã€‚</div></div></div></div><footer><div class="footer-content"><div class="copyright-info"><i class="fa-regular fa-copyright fa-xs"></i><span>2022 - 2025 </span><a href="/about">Schwertlilien</a><i class="fa-solid fa-cat fa-sm"></i><span>Powered by </span><a href="https://hexo.io/" target="_blank">Hexo</a><span> &amp; </span><a href="https://github.com/chanwj/hexo-theme-meow" target="_blank" title="v2.1.0">Theme Meow</a></div><div class="pageview-site"><span id="busuanzi_container_site_pv">æ€»è®¿é—®é‡ : <span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner"></i></span></span><span id="busuanzi_container_site_uv">æ€»è®¿å®¢æ•° : <span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner"></i></span></span></div></div></footer>
<script>const GLOBAL_CONFIG = {
  comment: { theme: 'preferred_color_scheme'}
}
</script>
<script src="/js/third-party/darkmode.js"></script>
<script>var options = {
  dark: '/css/darkmode.css',
  startAt: '24:00',
  endAt: '06:00',
  checkSystemScheme: 'false',
  saveOnToggle: 'true'
};
var darkMode = new DarkMode(options);
// change comment theme synchronously åŒæ­¥ä¿®æ”¹è¯„è®ºåŒºä¸»é¢˜
if (darkMode.getMode() == "dark" && (true || true)) {
  if (document.getElementById('comment')) {
    document.getElementById('comment').getElementsByTagName('script')[0].setAttribute('data-theme', 'noborder_dark');
  }
}
</script><script>if (localStorage.getItem('font-size')) {
  document.querySelector('.post-content').style.fontSize = localStorage.getItem('font-size') + 'px';
}
</script>
<script src="/js/theme/tool-bar.js"></script>


<script src="/js/theme/menu.js"></script>


<script src="/js/third-party/clipboard.min.js"></script>


<script src="/js/theme/copy.js"></script>
<script>copyCode();
</script>
<script src="/js/jquery-3.7.1.min.js"></script>


<script src="/js/theme/search.js"></script>
<script>searchFunc('/search.xml', 'search-input', 'search-result');
</script></body></html>